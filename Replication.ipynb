{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "d3ef53f5-a464-4774-aa85-3d4f63955e2d",
      "cell_type": "code",
      "source": "\"\"\"\nAuthor: Reggie Hyde\nDate: Spring 2025\nDescription: This Python jupyter notebook replicates the results of \nMulti-Objective Portfolio Optimization via Machine Learning: Accounting for Practicality Across Investor Types\nThe notebook is split into 2 main parts:\n1) Replicating Gu et. al's NN3 model from Empirical Asset Pricing via Machine Learning (2020)\n2) Proposing a framework for multi-objective portfolio optimization that accounts for practicality across investor types, driven by risk premia\npredictions from Gu et. al's NN3 model.\n\"\"\"\n\n#Thank you! Please reach out with any questions at reginaldjhyde@gmail.com or hyde9698@stthomas.edu\n\n#Library imports\nimport os #For saving checkpoints, accessing characteristic data, etc.\nimport glob #For file searching\nimport numpy as np #For matrix math\nimport pandas as pd #For tabular data\nimport gc #For manual garbage collecting for memory management (optional, might be useful for those with limited computational power)\nimport pickle #For saving data\nfrom itertools import product #For using cartesian product\n\n#Pytorch library imports\nimport torch #For training neural network\nimport torch.nn as nn #For accessing built-in common machine learning functions such as ReLU\nimport torch.optim as optim #For accessing built-in common machine learning optimizers (we use Adam)\nfrom torch.utils.data import TensorDataset, DataLoader #For training data management\nfrom torch.cuda.amp import autocast, GradScaler #For accessing mixed-precision training\n\n#Scikit-learn imports\nfrom sklearn.model_selection import train_test_split, ParameterGrid #For model selection functionality\n\nprint(\"Libraries imported successfully.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Libraries imported successfully.\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "21ad75cd-c748-4fdd-8b88-fea7588070da",
      "cell_type": "code",
      "source": "\"\"\"\nTHE FOLLOWING CELLS REPLICATE GU ET. AL'S NN3 NEURAL NETWORK\nFirst, we must replicate their training, validation, and out-of-sample testing time window specifications.\nGu. et al implement a validation rolling window of sorts.\n\"\"\"\n\nvalidation_length = 12 #Define the length of the validation period, in years \n\nyears = np.arange(1987, 2017) #Create an array of years from 1987 to 2016 (end is non-inclusive)\n\nestimation_periods = pd.DataFrame({ #Build a DataFrame with time period specifications\n    'oos_year': years, #Out-of-sample year for testing\n    'validation_end': years-1, #Validation period ends one year before the OOS year\n    'validation_start': years - validation_length, #Validation starts 'validation_length' years before OOS year\n    'training_start': 1957, #Training period starts from 1957\n    'training_end': years - validation_length - 1 #Training ends one year before validation starts\n})\n\nprint(estimation_periods.head()) #Print out the first 5 periods to verify replication",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "   oos_year  validation_end  validation_start  training_start  training_end\n,0      1987            1986              1975            1957          1974\n,1      1988            1987              1976            1957          1975\n,2      1989            1988              1977            1957          1976\n,3      1990            1989              1978            1957          1977\n,4      1991            1990              1979            1957          1978\n"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "fab2f84c-8636-4ae7-baf6-78e6a334ba20",
      "cell_type": "code",
      "source": "class NN3(nn.Module): #This class defines the neural network model architecture for the NN3 model\n    def __init__(self, input_dim):\n        super(NN3, self).__init__()\n        self.net = nn.Sequential( #Stacks layers in following order\n            nn.Linear(input_dim, 32), #The network compresses the 920 interaction term inputs into 32 neurons in the first hidden layer\n            nn.ReLU(), #ReLU activation function\n            nn.Linear(32, 16), #The network compresses the 32 neurons in the first hidden layer into the 16 neurons in the second hidden layer\n            nn.ReLU(), #ReLU activation function\n            nn.Linear(16, 8), #The network compresses the 16 neurons in the second hidden layer into the 8 neurons in the third hidden layer\n            nn.ReLU(), #ReLU activation function\n            nn.Linear(8, 1) #The network compresses the 8 neurons in the third hidden layer into the output (predicted risk premia)\n        )\n        \n    def forward(self, x): #Specifies forward pass\n        return self.net(x)\n\nprint(\"NN3 model defined.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "NN3 model defined.\n"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "bec86e9a-9c3b-459e-a859-74f08106c3d5",
      "cell_type": "code",
      "source": "def train_model(model, train_loader, val_loader, lr, weight_decay, epochs=100): #This function trains the model for a given OOS year for a given combination of hyperparameters\n    device = torch.device(\"cuda\") #Find GPU. Note: Training this model on a CPU alone for all the OOS years will be a significant undertaking.\n    model = model.to(device) #Designate GPU to train model\n    \n    criterion = nn.MSELoss() #Specify loss function as Mean Squared Error\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) #Specify optimizer as Adam with current hyperparameters\n    scaler = torch.amp.GradScaler() #Specify scaler as gradient scaler for mixed-precision training\n    \n    #INITIAL TRAINING\n    for epoch in range(epochs): #Iterate through n number of epochs, default will be 100\n        model.train() #Specify that we're training the model\n        running_loss = 0.0 \n        \n        for xb, yb in train_loader: #Iterate through each small batch of input data and their corresponding outputs\n            xb, yb = xb.to(device), yb.to(device) #Move batch of data onto the GPU\n            optimizer.zero_grad() #Clear gradients from last backward pass\n\n            with torch.amp.autocast(\"cuda\"): #Specify automatic mixed precision to GPU\n                loss = criterion(model(xb).squeeze(), yb) #Compute training loss\n                \n            scaler.scale(loss).backward() #Compute gradients for this batch\n            scaler.step(optimizer) #Update weights based on these gradients\n            scaler.update() #Dynamically adjust scaling factor\n            \n            running_loss += loss.item() #Add this batch's loss to total for later average calculation\n\n        avg_loss = running_loss / len(train_loader) #Calculate average loss across all batches\n        print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {avg_loss:.6f}\")\n\n    #VALIDATION (for evaluating best set of hyperparameters later)\n    model.eval() #Specify that we're now validating the model\n    val_losses = [] #Initialize empty array of batch validation losses\n    with torch.no_grad(): #Turn off gradient tracking\n        for xb, yb in val_loader: #Iterate through each small batch of validation data and their corresponding outputs\n            xb, yb = xb.to(device), yb.to(device) #Move batch of data onto the GPU\n\n            with torch.amp.autocast(\"cuda\"): #Specify automatic mixed precision to GPU\n                val_loss = criterion(model(xb).squeeze(), yb) #Compute validation loss\n\n            val_losses.append(val_loss.item()) #Add this batch's loss to array for later average calculation\n    \n    return model, np.mean(val_losses) #Return the trained model and the validation loss average\n\nprint(\"Training function defined.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Training function defined.\n"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "1ae29b35-8e85-4c98-8354-e0c2f4676b7b",
      "cell_type": "code",
      "source": "param_grid = { #Define the possible values for learning rate and weight decay, based on best practices (citations required)\n    'learning_rate': [0.001, 0.01],\n    'penalty': [1e-5, 1e-3],\n}\ngrids = list(ParameterGrid(param_grid)) #Define the hyperparameter grid for learning rate and weight decay\nprint(\"Hyperparameter grid defined.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Hyperparameter grid defined.\n"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "27aadf7d-0d90-41d9-93a4-7dde638cd5b9",
      "cell_type": "code",
      "source": "#ACKNOWLEDGEMENT: Characteristics data was prepared with the help of a Tidy Finance blog post by Stefan Voigt (https://www.tidy-finance.org/blog/gu-kelly-xiu-replication/)\n#This cell loads all the prepared characteristic data\ndownloads_folder = r\"PATH_OF_YOUR_PREPARED_CHARACTERISTICS\" #Specify path of downloads folder\nfile_paths = glob.glob(os.path.join(downloads_folder, \"characteristics_prepared_*/year=*/part-0.parquet\")) #Locate all .parquet files by year. (This file path format is how the Tidy Finance blog post does it.)\nall_data = {}  #Initialize an empty dictionary to store data keyed by year\n\nfor path in file_paths_for_neural_net_replication :  #Iterate over each file path of characteristics data by year \n    chunk = pd.read_parquet(path)  #Read the parquet file into a pandas DataFrame\n    year = int(path.split('=')[-1].split('\\\\')[0])  #Extract the year from the file path\n    all_data[year] = chunk  #Store the year's data in the dictionary with the corresponding year as the key\n    print(f\"Loaded data for year: {year}\")\n\ncharacteristics_prepared_all = pd.concat(all_data.values(), ignore_index=True) #Concatenate all yearly DataFrames into one DataFrame\ncharacteristics_prepared_all['year'] = pd.to_datetime(characteristics_prepared_all['date'], errors='coerce').dt.year #Create column in DataFrame for just year\n\ngc.collect() #Manually free up memory",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Loaded data for year: 1957\n,Loaded data for year: 1958\n,Loaded data for year: 1959\n,Loaded data for year: 1960\n,Loaded data for year: 1961\n,Loaded data for year: 1962\n,Loaded data for year: 1963\n,Loaded data for year: 1964\n,Loaded data for year: 1965\n,Loaded data for year: 1966\n,Loaded data for year: 1967\n,Loaded data for year: 1968\n,Loaded data for year: 1969\n,Loaded data for year: 1970\n,Loaded data for year: 1971\n,Loaded data for year: 1972\n,Loaded data for year: 1973\n,Loaded data for year: 1974\n,Loaded data for year: 1975\n,Loaded data for year: 1976\n,Loaded data for year: 1977\n,Loaded data for year: 1978\n,Loaded data for year: 1979\n,Loaded data for year: 1980\n,Loaded data for year: 1981\n,Loaded data for year: 1982\n,Loaded data for year: 1983\n,Loaded data for year: 1984\n,Loaded data for year: 1985\n,Loaded data for year: 1986\n,Loaded data for year: 1987\n,Loaded data for year: 1988\n,Loaded data for year: 1989\n,Loaded data for year: 1990\n,Loaded data for year: 1991\n,Loaded data for year: 1992\n,Loaded data for year: 1993\n,Loaded data for year: 1994\n,Loaded data for year: 1995\n,Loaded data for year: 1996\n,Loaded data for year: 1997\n,Loaded data for year: 1998\n,Loaded data for year: 1999\n,Loaded data for year: 2000\n,Loaded data for year: 2001\n,Loaded data for year: 2002\n,Loaded data for year: 2003\n,Loaded data for year: 2004\n,Loaded data for year: 2005\n,Loaded data for year: 2006\n,Loaded data for year: 2007\n,Loaded data for year: 2008\n,Loaded data for year: 2009\n,Loaded data for year: 2010\n,Loaded data for year: 2011\n,Loaded data for year: 2012\n,Loaded data for year: 2013\n,Loaded data for year: 2014\n,Loaded data for year: 2015\n,Loaded data for year: 2016\n"
        },
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 6
    },
    {
      "id": "d065a715-0b69-4cb7-ae84-d48612ca68a8",
      "cell_type": "code",
      "source": "#This cell trains the model for a given j OOS year. \n#For the purposes of strictly replicating Gu et. al, full window is only range(0, 30, 1)\n\nneural_net_replication_range = list(range(0, 30, 1)) #Range for replicating neural network from Gu. et al\n\nfor j in neural_net_replication_range: #Iterate through each OOS year. 30 years = to 2016\n    print(f\"\\nProcessing iteration: {j+1}\")\n    \n    split_dates = estimation_periods.iloc[j] #Get split dates for this OOS years\n    cutoff_year = split_dates['validation_end'] #Get end year of validation window \n    data = characteristics_prepared_all[characteristics_prepared_all['year'] <= cutoff_year] #Get data for training + validation\n    train = data[data['year'] < split_dates['training_end']] #Define training set\n    validation = data[(data['year'] >= split_dates['validation_start']) &  (data['year'] <= split_dates['validation_end'])] #Define validation set\n   \n    columns_to_drop = ['permno', 'date', 'mktcap_lag', 'year'] #Define list of columns to drop from predictors\n    X_train = train.drop(columns=columns_to_drop + ['ret_excess']) #Prepare training inputs\n    y_train = train['ret_excess'] #Prepare training outputs (ret_excess column contains risk premia)\n   \n    X_val = validation.drop(columns=columns_to_drop + ['ret_excess']) #Prepare validation inputs\n    y_val = validation['ret_excess'] #Prepare validation outputs\n\n    X_train = X_train.fillna(X_train.mean()) #Fill any N/A values in training inputs with the mean of its column\n    X_val = X_val.fillna(X_val.mean()) #Fill any N/A values in training inputs with the mean of its column\n   \n    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32) #Convert training inputs into PyTorch tensor\n    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32) #Convert training output into PyTorch tensor\n    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32) #Convert validation inputs into PyTorch tensor\n    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32) #Convert validation output into PyTorch tensor\n\n    train_ds = TensorDataset(X_train_tensor, y_train_tensor) #Create TensorDataaset for training data (inputs, output)\n    val_ds = TensorDataset(X_val_tensor, y_val_tensor) #Create TensorDataaset for validation data (inputs, output)\n \n    train_loader = DataLoader(train_ds, batch_size=10000, shuffle=True) #Create DataLoader for training data. Batch size is adjustable based on computational power/memory\n    val_loader = DataLoader(val_ds, batch_size=10000) #Create DataLoader for validation data. Batch size is adjustable based on computational power/memory\n\n    best_loss = float('inf') #Initialize best loss float\n    best_model = None #Initialize best model container\n    best_params = None #Initialize best model parameters container\n   \n    for params in grids: #Loop over all hyperparameter combinations in the grid\n        model = NN3(input_dim=X_train.shape[1]) #Create model object using NN3 class\n        trained_model, val_loss = train_model(model, train_loader, val_loader, lr=params['learning_rate'], weight_decay=params['penalty']) #Call \"train_model()\"\n        if val_loss < best_loss: #If this train had a superior loss, save model as best model so far\n            best_loss = val_loss\n            best_model = trained_model\n            best_params = params\n      \n    model_path = f\"NN3_fitted_model_{split_dates['oos_year']}.pt\" #Define file path for saving best model\n    torch.save(best_model.state_dict(), model_path) #Save the best model for this OOS year to this file\n    print(f\"Saved model to {model_path}\")\n",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n,Processing iteration: 1\n"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m X_train = train.drop(columns=columns_to_drop + [\u001b[33m'\u001b[39m\u001b[33mret_excess\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;66;03m#Prepare training inputs\u001b[39;00m\n\u001b[32m     17\u001b[39m y_train = train[\u001b[33m'\u001b[39m\u001b[33mret_excess\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m#Prepare training outputs (ret_excess column contains risk premia)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m X_val = \u001b[43mvalidation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns_to_drop\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mret_excess\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Prepare validation inputs\u001b[39;00m\n\u001b[32m     20\u001b[39m y_val = validation[\u001b[33m'\u001b[39m\u001b[33mret_excess\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m#Prepare validation outputs\u001b[39;00m\n\u001b[32m     22\u001b[39m X_train = X_train.fillna(X_train.mean()) \u001b[38;5;66;03m#Fill any N/A values in training inputs with the mean of its column\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5435\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5443\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5445\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5579\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5583\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5587\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5588\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4786\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4788\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4791\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4869\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4866\u001b[39m     new_axis = axis.take(indexer)\n\u001b[32m   4868\u001b[39m bm_axis = \u001b[38;5;28mself\u001b[39m.ndim - axis_num - \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4869\u001b[39m new_mgr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4872\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4875\u001b[39m \u001b[43m    \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4876\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4877\u001b[39m result = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_mgr, axes=new_mgr.axes)\n\u001b[32m   4878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequested axis not found in manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     new_blocks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    687\u001b[39m     new_blocks = [\n\u001b[32m    688\u001b[39m         blk.take_nd(\n\u001b[32m    689\u001b[39m             indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    696\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[39m, in \u001b[36mBaseBlockManager._slice_take_blocks_ax0\u001b[39m\u001b[34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[39m\n\u001b[32m    841\u001b[39m                     blocks.append(nb)\n\u001b[32m    842\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m                 nb = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m                 blocks.append(nb)\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1304\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1306\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1312\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1314\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1315\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[32m    165\u001b[39m     out = out.T\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "id": "cff47081-f575-4ff5-b91d-aa499b4af4b4",
      "cell_type": "code",
      "source": "#This cell makes OOS predictions using trained models for each OOS year\n#Then, portfolio is constructed by going long on the top 10 percent of stocks by predicted risk premia and going short on the bottom 10 percent\n#This is called the H-L portfolio. The table printed out at the bottom replicates Gu et. al methodology\n\nneural_net_replication_oos_range = list(range(1987, 2017)) #1987-2016, OOS range for replicating Table 7 in Gu et. al paper\noos_predictions = [] #Initialize empty array for OOS predictions\n\nfor oos_year in range(1986, 2017):  #Iterate through each OOS year\n    print(f\"Processing OOS year: {oos_year}\")\n\n    feature_cols = [col for col in characteristics_prepared_all.columns if col not in ['permno', 'date', 'mktcap_lag', 'ret_excess', 'year']] #Get all predictor columns\n\n    base_dir = r\"YOUR_BASE_DIRECTORY_STORING_FITTED_MODELS\"\n    model_path = rf\"{base_dir}\\NN3_fitted_model_{oos_year}.pt\" #Load the saved fitted model for this OOS year\n    model = NN3(input_dim=len(feature_cols)) #Create model object using NN3 class\n    model.load_state_dict(torch.load(model_path)) #Load model\n    model.eval() #Specify model to be in eval mode, which is for validation or testing the model with out-of-sample data\n\n    oos_data = characteristics_prepared_all[characteristics_prepared_all['year'] == oos_year].copy() #Get characteristics data from this OOS year\n    oos_data.dropna(subset=['ret_excess'], inplace=True) #Drop any N/A values in the OOS data (shouldn't be any, but procedural assurance)\n\n    X_oos = oos_data[feature_cols] #Get OOS input data in the feature columns\n    X_oos = X_oos.fillna(X_oos.mean()) #Fill any N/A values in OOS inputs with the mean of its column\n    X_oos_tensor = torch.tensor(X_oos.values, dtype=torch.float32) #Convert OOS input data to Tensor\n\n    with torch.no_grad(): #Specify gradient tracking is not necessary\n        preds = model(X_oos_tensor).squeeze() #Make model predictions\n\n    oos_chunk = oos_data[['permno', 'date', 'mktcap_lag', 'ret_excess']].copy() #Get chunk of non-feature column data for this OOS year\n    oos_chunk['predictions'] = preds.numpy() #Add the predictions column to this chunk for this OOS year\n    oos_predictions.append(oos_chunk) #Add this year's chunk to the oos_predictions array\n\noos_predictions = pd.concat(oos_predictions, ignore_index=True) #Concatenate all year's data into one DataFrame\n\n#Now, we will construct portfolios to replicate Gu et. al\ndef assign_portfolio_by_month(df, sorting_variable, n_portfolios=10): #This function will sort the DataFrame into 10 deciles based on a sorting variable, which in our case will be predicted risk premia\n    df['portfolio'] = df.groupby('date')[sorting_variable].transform(lambda x: pd.qcut(x, q=n_portfolios, labels=False, duplicates='drop') + 1)\n    return df\n\n#Sort stocks into decile portfolios based on raw predicted returns (no value-weighting at this stage)\nml_portfolios = oos_predictions.copy() #Copy OOS predictions data \nml_portfolios = assign_portfolio_by_month(ml_portfolios, sorting_variable='predictions', n_portfolios=10) #Construct portfolio decile groups using assign_portfolio_by_month() function, sorting variable is predicted risk premia\nml_portfolios['portfolio'] = ml_portfolios['portfolio'].astype('category') #Convert numerical portfolio decile column into categorical variable\nml_portfolios.dropna(subset=['ret_excess'], inplace=True) #Removes rows where ret_excess has NaN values\n\ndef weighted_mean(df, value, weight): #This function calculates a weighted mean based off of a value and a corresponding weight.\n    return (df[value] * df[weight]).sum() / df[weight].sum() #In our case, value = predicted risk premia (in percentage), weight = market capitalization \n    #(to calculate realistic returns, we care more about high percentage gains/losses in bigger companies)\n\n# Value-weighted monthly returns per decile\nml_portfolios_summary = ( #Calculate value-weighted monthly actual realized average returns and value-weighted predicted average returns and add columns to table\n    ml_portfolios.groupby(['portfolio', 'date'], observed=False)\n    .apply(lambda x: pd.Series({\n        'predictions': weighted_mean(x, 'predictions', 'mktcap_lag'), #Add predicted average returns column (value-weighted)\n        'ret_excess': weighted_mean(x, 'ret_excess', 'mktcap_lag') #Add realized average returns column (value-weighted)\n    }))\n    .reset_index()\n)\n\n#This code constructs the H-L portfolio as described in Gu et. al (long on top 10 percent predicted, short on bottom 10 percent predicted)\n\np10 = ml_portfolios_summary[ml_portfolios_summary['portfolio'] == 10].copy() #Copy the 10th decile (top decile in terms of raw percentage predicted risk premia)\np1 = ml_portfolios_summary[ml_portfolios_summary['portfolio'] == 1].copy() #Copy the 1st decile (bottom decile in terms of raw percentage predicted risk premia)\n\nhl_merge = pd.merge(p10, p1, on='date', suffixes=('_10', '_1')) #Create DF by merging 10th decile and 1st decile \nhl_merge['predictions'] = hl_merge['predictions_10'] - hl_merge['predictions_1'] #Long on 10th decile stocks, short on 1st decile stocks to measure theoretical value-weighted portfolio return (for predicted risk premia)\nhl_merge['ret_excess'] = hl_merge['ret_excess_10'] - hl_merge['ret_excess_1'] #Long on 10th decile stocks, short on 1st decile stocks to measure theoretical value-weighted portfolio return (for realized risk premia)\nhml_portfolio = hl_merge[['date', 'predictions', 'ret_excess']].copy() #Add date, value-weighted predictions, and value-weighted realized returns columns to H-L\nhml_portfolio['portfolio'] = 'H-L' #Name: H-L\n\nml_portfolios_summary_updated = pd.concat([ml_portfolios_summary, hml_portfolio], ignore_index=True) #Combine 10 deciles with H-L portfolio in the table\n\noverall_summary = ml_portfolios_summary_updated.groupby('portfolio').agg( #Construct overall summary table\n    Pred=('predictions', 'mean'),\n    Avg=('ret_excess', 'mean'),\n    Std=('ret_excess', 'std')\n).reset_index()\n\noverall_summary['Pred'] *= 100 #Convert predicted risk premia to percentage format\noverall_summary['Avg'] *= 100 #Convert realized risk premia to percentage format\noverall_summary['Std'] *= 100 #Convert standard deviation of realized risk premia to percentage format\noverall_summary['SR'] = ( #Calculate annualized Sharpe ratio\n    overall_summary['Avg'] / overall_summary['Std']\n) * (12 ** 0.5)\n\n#Display table\npd.set_option('display.max_rows', None) \npd.set_option('display.max_columns', None)\nprint(overall_summary)\n",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Processing OOS year: 1986\n,Processing OOS year: 1987\n,Processing OOS year: 1988\n,Processing OOS year: 1989\n,Processing OOS year: 1990\n,Processing OOS year: 1991\n,Processing OOS year: 1992\n,Processing OOS year: 1993\n,Processing OOS year: 1994\n,Processing OOS year: 1995\n,Processing OOS year: 1996\n,Processing OOS year: 1997\n,Processing OOS year: 1998\n,Processing OOS year: 1999\n,Processing OOS year: 2000\n,Processing OOS year: 2001\n,Processing OOS year: 2002\n,Processing OOS year: 2003\n,Processing OOS year: 2004\n,Processing OOS year: 2005\n,Processing OOS year: 2006\n,Processing OOS year: 2007\n,Processing OOS year: 2008\n,Processing OOS year: 2009\n,Processing OOS year: 2010\n,Processing OOS year: 2011\n,Processing OOS year: 2012\n,Processing OOS year: 2013\n,Processing OOS year: 2014\n,Processing OOS year: 2015\n,Processing OOS year: 2016\n,   portfolio      Pred       Avg       Std        SR\n,0        1.0 -1.418601 -0.411674  7.201218 -0.198033\n,1        2.0 -0.557768  0.314269  6.262479  0.173839\n,2        3.0 -0.123418  0.487294  5.252859  0.321356\n,3        4.0  0.186873  0.538562  4.615785  0.404186\n,4        5.0  0.445405  0.671282  4.169990  0.557649\n,5        6.0  0.661122  0.773409  4.173835  0.641896\n,6        7.0  0.902329  1.039038  4.590690  0.784050\n,7        8.0  1.234602  1.019703  4.858195  0.727092\n,8        9.0  1.785287  1.248123  6.563929  0.658695\n,9       10.0  2.696712  2.078179  7.464866  0.964387\n,10       H-L  4.059113  2.601917  5.686223  1.585113\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "C:\\Users\\Reggie Hyde\\AppData\\Local\\Temp\\ipykernel_3000\\417990182.py:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n,  .apply(lambda x: pd.Series({\n"
        }
      ],
      "execution_count": 8
    },
    {
      "id": "b061f727-bc9a-4e5b-966e-bb9961582d60",
      "cell_type": "code",
      "source": "\"\"\"\nTHE FOLLOWING CELLS ARE THE CONTRIBUTION OF THIS PAPER\nThe following cells propose a portfolio optimization model, which aims to solve a customizable objective function using Sequential Least Squares Programming (SLSQP)\n\"\"\"\nfrom datetime import datetime #For dealing with dates\nfrom scipy.optimize import minimize #For using minimize function, which is a general-purpose nonlinear solver\nfrom sklearn.preprocessing import StandardScaler #For calculating Z-scored arbitrage difficulty penalty\nfrom dateutil.relativedelta import relativedelta #For date arithmetic\nimport time #For dealing with time\n\nOOS_OPT_START_YEAR = 1987 #Optimization year range begins here\n\nCHARACTERISTIC_IMPORTANCE = { #Define importance weight for each stock-level DTA score interaction term (These importance weights were determined from analysis of existing literature)\n    \"ill\": 1.00,\n    \"baspread\": 1.00,\n    \"idiovol\": 0.70,\n    \"turn\": 0.50,\n}\n\nMACRO_IMPORTANCE = { #Define importance weight for each macroeconomic DTA score interaction term (These importance weights were determined from analysis of existing literature)\n    \"svar\": 1.00,\n    \"tbl\": 0.70\n}\n\n#Cartesian product of stock-level characteristics: (ill, idiovol, baspread, turn) and macroeconomic characteristics: (svar, tbl)\nDTA_INTERACTION_MAP = { #Map characteristics with their two components\n    \"characteristic_ill_x_macro_svar\": (\"ill\", \"svar\"),\n    \"characteristic_ill_x_macro_tbl\": (\"ill\", \"tbl\"),\n    \"characteristic_idiovol_x_macro_svar\": (\"idiovol\", \"svar\"),\n    \"characteristic_idiovol_x_macro_tbl\": (\"idiovol\", \"tbl\"),\n    \"characteristic_baspread_x_macro_svar\": (\"baspread\", \"svar\"),\n    \"characteristic_baspread_x_macro_tbl\": (\"baspread\", \"tbl\"),\n    \"characteristic_turn_x_macro_svar\": (\"turn\", \"svar\"),\n    \"characteristic_turn_x_macro_tbl\": (\"turn\", \"tbl\"),\n}\n\nDTA_WEIGHTS = {} #Initialize empty dictionary for DTA_WEIGHTS\n\nfor interaction_term, (characteristic, macro) in DTA_INTERACTION_MAP.items(): #Iterate through each interaction term\n    weight = CHARACTERISTIC_IMPORTANCE[characteristic] * MACRO_IMPORTANCE[macro] #Calculate weight of each interaction term\n    DTA_WEIGHTS[interaction_term] = weight #Assign weight to interaction term\n\nWINDOW_MONTHS = 12 #Define trailing window length for covariance matrix calculating (look 1 year back)\nEPSILON = 1e-6 #Define smoothing constant for turnover penalty\n\nprint(\"Optimization Initialization completed.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Optimization Initialization completed.\n"
        }
      ],
      "execution_count": 9
    },
    {
      "id": "1717dcc3-3ace-4318-9196-6e45f6126d81",
      "cell_type": "code",
      "source": "def compute_di(df): #Function for computing the arbitrage difficulty penalty in the optimization problem at a given t \n    Z = df[list(DTA_WEIGHTS.keys())].copy() #Extract the columns of the interaction terms involved in calculating the arbitrage difficulty penalty\n\n    Z = Z.dropna() #Drop any rows with N/A values in these columns\n    Z_scaled = pd.DataFrame( #Standardize each interaction term to ensure fair weighting across terms\n        StandardScaler().fit_transform(Z), #Applies (x - mean) / std to each column\n        index=Z.index,\n        columns=Z.columns\n    )\n\n    weight_vector = np.array([DTA_WEIGHTS[col] for col in Z_scaled.columns]) #Get a weight vector aligned by columns/keys\n    S = Z_scaled.values @ weight_vector #Calculate row-wise dot product\n    D = (S - S.min()) / (S.max() - S.min()) #Normalize S to [0, 1]\n    return pd.Series(D, index=Z_scaled.index) #Return final D_i vector\n\nprint(\"Arbitrage difficulty function defined.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Arbitrage difficulty function defined.\n"
        }
      ],
      "execution_count": 10
    },
    {
      "id": "c5392a83",
      "cell_type": "code",
      "source": "def compute_cov_matrix(returns_df): #Function to calculate the covariance matrix of the stocks at a given t\n    return np.cov(returns_df.T) #Return covariance matrix (transpose returns so that np.cov treats columns as each stock)\n    \nprint(\"Covariance matrix function defined.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Covariance matrix function defined.\n"
        }
      ],
      "execution_count": 11
    },
    {
      "id": "20286676",
      "cell_type": "code",
      "source": "def optimization_objective(w, mu, Sigma, D, w_prev, lambda_t, lambda_r, lambda_a): #This function defines the optimization objective expression to be minimized\n    #w = current weight vector (currently being optimized), mu = predicted risk premia output from model, Sigma = covariance matrix, D = computed arbitrage difficulty score [0, 1], w_prev: previous month's weights, \n    #lambda_t: penalty weight for turnover, lambda_r: Penalty weight for risk, lambda_a: Penalty weight for arbitrage difficulty\n\n    turnover = np.sum(np.sqrt((w - w_prev)**2 + EPSILON)) #Define smoothed turnover penalty that approximates |w_i - w_prev_i|.\n    #Smoothed to insure differentiability (absolute value functions are indifferentiable at x=0, which is non-friendly to optimzation solvers)\n\n    risk = w @ Sigma @ w #Define risk term as the transpose of the current weights multiplied by the covariance matrix, multiplied by the current weights\n    #In this case, the weights vector in the leftmost operand are implicitly transposed.\n\n    arb = np.sum(D * w**2) #Define weighted quadratic term discouraging heavy allocation to difficult-to-arbitrage stocks\n\n    percentage_returns = w @ mu #Calculate the hypothetical percentage returns with the current set of weights\n    return -(percentage_returns - lambda_t * turnover - lambda_r * risk - lambda_a * arb) #Return negative of the reward, since we use a minimizer\n\nprint(\"Optimization objective function defined.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Optimization objective function defined.\n"
        }
      ],
      "execution_count": 12
    },
    {
      "id": "e1e3f184-001f-486b-9caa-844755b84db2",
      "cell_type": "code",
      "source": "def optimize_weights(mu, Sigma, D, w_prev, lambda_t, lambda_r, lambda_a, bounds=None): #Function to optimize the portfolio weights for one month (t)\n    #mu = predicted risk premia output from model, Sigma = covariance matrix, D = computed arbitrage difficulty score [0, 1], w_prev: previous month's weights,\n    #lambda_t: penalty weight for turnover, lambda_r: Penalty weight for risk, lambda_a: Penalty weight for arbitrage difficulty, bounds = Optional box constraints for each weight (could add some bounds to customize optimization)\n\n    n = len(mu) #Get the current number of stocks we're considering\n    w0 = np.zeros(n) #Initialize weights vector with all zeros\n    constraints = ({ #Add constraint that Weights must sum to zero\n        'type': 'eq',\n        'fun': lambda w: np.sum(w)\n    })\n\n    def optimization_callback(w): #Function to be called at each iteration of SLSQP that displays the mean, max, and min of the current weight vector\n        print(f\"   â†³ Iteration: mean(w)={w.mean():.4e}, max={w.max():.4f}, min={w.min():.4f}\")\n\n    #This is where we start the optimization function\n    print(\"Starting minimize()...\")\n    start_time = time.time() #Record start time of function\n\n    res = minimize( #Call minimize() from scipy.optimize with the following arguments:\n        optimization_objective, #Pass in the optimization objective function\n        w0, #Pass in initial w0 of all zeros\n        args=(mu, Sigma, D, w_prev, lambda_t, lambda_r, lambda_a), #Pass in tuple of additional arguments that are passed to the optimization objective, after w0.\n        constraints=constraints, #Pass in constraints\n        bounds=bounds, #Pass in bounds\n        method='SLSQP', #Specify that we want to use SLSQP\n        callback=optimization_callback, #Pass in callback function that will print out a small report after each iteration of optimization\n        options={ #Pass in a set of options\n            'disp': True, #We want final summary after convergence\n            'maxiter': 100, #Must be large number such as 100 for meaningful results (could benefit from increasing, potentially.)\n            'ftol': 1e-5 #Change of function value must be less than 10^-5 for us to stop function early\n        }\n    )\n\n    elapsed = time.time() - start_time #Calculate elapsed time from minimize() call\n    print(f\"Optimization finished in {elapsed:.3f} seconds | Success: {res.success} | Iterations: {res.nit}\\n\") #Display elapsed seconds, success, and number of iterations\n    if res.success : #If optimization problem succeeded\n        return res.x #Return optimized weights\n    else :\n        print(\"------------FAILED: RETURNED ALL-ZERO WEIGHTS MATRIX DUE TO FAILURE TO OPTIMIZE------------\") #Print failure message\n        return np.zeros(n) #Return all-zero weights matrix\n\nprint(\"Optimization function defined.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Optimization function defined.\n"
        }
      ],
      "execution_count": 13
    },
    {
      "id": "df1fedb6-8f70-47e0-bae9-9a01d4102ec5",
      "cell_type": "code",
      "source": "def get_initial_HL_weights(mu_series): #Function to construct the initial w_prev vector using the H-L portfolio present in Gu et. al\n    #mu_series = series object that contains permno as the ID's for predicted risk premia (series is similar to a dictionary)\n    n = len(mu_series) #Get number of stocks currently working with\n    n_in_decile = int(n * 0.1) #Get number of stocks in a decile \n\n    mu_sorted = mu_series.sort_values(ascending=False) #Sort mu_series in descending order by its values, in this case predicted risk premia\n    top_permnos = mu_sorted.iloc[:n_in_decile].index #Store top 10 percent of stocks in terms of predicted risk premia in top_permnos\n    bottom_permnos = mu_sorted.iloc[-n_in_decile:].index #Store bottom 10 percent of stocks in terms of predicted risk premia in top_permnos\n\n    long_weight = 1.0 / n_in_decile #Equal weight long position\n    short_weight = -1.0 / n_in_decile #Equal weight short position\n\n    weights = pd.Series(0.0, index=mu_series.index) #Initialize weights vector with all zeros\n    weights.loc[top_permnos] = long_weight #Assign long_weight to top 10 percent\n    weights.loc[bottom_permnos] = short_weight #Assign short_weight to bottom 10 percent\n\n    assert np.isclose(weights.sum(), 0.0), \"Weights do not sum to zero\" #Ensure the sum of the initial weights is very close to 0\n    \n    return weights #Return the H-L portfolio weights\n\nprint(\"Initial H-L portfolio function defined.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Initial H-L portfolio function defined.\n"
        }
      ],
      "execution_count": 14
    },
    {
      "id": "65f97bd8",
      "cell_type": "code",
      "source": "#This cell prepares for the DataFrame for the portfolio optimization function\ndf_base = characteristics_prepared_all.copy() #Define DataFrame for the optimization problem\ndf_base['date'] = pd.to_datetime(df_base['date']) #Convert date column in DF to datetime objects\noos_predictions['date'] = pd.to_datetime(oos_predictions['date']) #Convert date column in oos_predictions to datetime objects\n\ndf_base = pd.merge( #Merge DF with oos_predictions\n    df_base, \n    oos_predictions[['permno', 'date', 'predictions']], #We care about these 3 columns in the oos_predictions DF: permno (stock ID), date, and risk premia prediction\n    on=['permno', 'date'], #Merge on ID and date\n    how='inner'\n)\n\ndf_base = df_base[['permno', 'date', 'ret_excess'] + list(DTA_INTERACTION_MAP.keys()) + ['predictions']] #Filter DataFrame to only include permno, date, ret_excess, the arbitrage difficulty \ndf_base['month'] = df_base['date'].dt.to_period('M') #Add month column to DataFrame\n\nprint(\"DataFrame for portfolio optimization constructed.\")",
      "metadata": {},
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 19.7 GiB for an array with shape (923, 2870681) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#This cell prepares for the DataFrame for the portfolio optimization function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_base = \u001b[43mcharacteristics_prepared_all\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Define DataFrame for the optimization problem\u001b[39;00m\n\u001b[32m      3\u001b[39m df_base[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df_base[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;66;03m#Convert date column in DF to datetime objects\u001b[39;00m\n\u001b[32m      4\u001b[39m oos_predictions[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(oos_predictions[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;66;03m#Convert date column in oos_predictions to datetime objects\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:6811\u001b[39m, in \u001b[36mNDFrame.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m   6662\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   6663\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Self:\n\u001b[32m   6664\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6665\u001b[39m \u001b[33;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[32m   6666\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6809\u001b[39m \u001b[33;03m    dtype: int64\u001b[39;00m\n\u001b[32m   6810\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6811\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6812\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_item_cache()\n\u001b[32m   6813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(data, axes=data.axes).__finalize__(\n\u001b[32m   6814\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6815\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    601\u001b[39m         res._blklocs = \u001b[38;5;28mself\u001b[39m._blklocs.copy()\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1785\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1786\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1788\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1790\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2267\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2272\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2294\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2287\u001b[39m new_values: ArrayLike\n\u001b[32m   2289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[32m0\u001b[39m].dtype, np.dtype):\n\u001b[32m   2290\u001b[39m     \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[32m   2291\u001b[39m     \u001b[38;5;66;03m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[32m   2292\u001b[39m     \u001b[38;5;66;03m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[32m   2293\u001b[39m     \u001b[38;5;66;03m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2294\u001b[39m     new_values = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   2295\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2296\u001b[39m     bvals = [blk.values \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_core\\shape_base.py:292\u001b[39m, in \u001b[36mvstack\u001b[39m\u001b[34m(tup, dtype, casting)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    291\u001b[39m     arrs = (arrs,)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mMemoryError\u001b[39m: Unable to allocate 19.7 GiB for an array with shape (923, 2870681) and data type float64"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "id": "f3b87a0e-789c-434d-a5b3-9a1f24b1fa9b",
      "cell_type": "code",
      "source": "def run_portfolio_optimization(lambda_t, lambda_r, lambda_a, full_oos=True, test_years=None): #Function for running portfolio optimization for a set of lambda penalty terms\n    #lambda_t: penalty weight for turnover, lambda_r: Penalty weight for risk, lambda_a: Penalty weight for arbitrage difficulty\n    #full_oos: boolean that tells function if we are running through whole OOS window (1987-2016)\n    #test_years: if we are not running through the whole OOS window, this list specicies which years are to be tested\n    \n    monthly_dates = sorted(df_base['month'].unique()) #Get all unique monthly dates in whole DataFrame\n\n    start_month = pd.Period(f'{OOS_OPT_START_YEAR}-01', freq='M') #Make start_month pd Period object based on the optimization start year\n\n    if full_oos: #If caller wants to execute for the whole OOS period\n        opt_months = [m for m in monthly_dates if m >= start_month] #Define opt_months as any month later or equal to start_month\n    else: #If not\n        opt_months = [m for m in monthly_dates if m.year in test_years] #Define opt_months as any month in the set of test_years\n\n    print(\"Number of months to optimize: \", len(opt_months))\n    print(\"First month: \", opt_months[0])\n    print(\"Last month: \", opt_months[-1])\n    \n    weights_history = [] #Initialize weights_history to record the progression of weights for potential later analysis\n    returns_history = [] #Initialize returns_history to record performance of optimized portfolio\n\n    w_prev = pd.Series(dtype=float) #Initialize previous weights Series object\n    \n    for month in opt_months: #Iterate through each month in opt_months\n        current_date = pd.Timestamp(month.to_timestamp()) #Get current date in months in timestamp format\n        one_year_ago = pd.Timestamp((month - WINDOW_MONTHS).to_timestamp()) #Calculate the month one year ago in timestamp format\n\n        past_returns = df_base[(df_base['month'] >= one_year_ago.to_period('M')) & (df_base['month'] < month)] #Filter DataFrame to include data between one year ago and now \n        ret_matrix = past_returns.pivot_table(index='month', columns='permno', values='ret_excess', aggfunc='mean').dropna(axis=1) #Get matrix of past realized risk premia between one year ago and now\n\n        if ret_matrix.shape[1] < 10 or ret_matrix.shape[0] < 12: #If there is not enough data for covariance matrix (SAFEGUARD, THIS SHOULD NEVER BE TRUE)\n            print(f\"------------Skipping {current_date.strftime('%Y-%m')} â€” insufficient data------------\") \n            continue\n\n        aligned_permnos = [int(p) for p in ret_matrix.columns.tolist()] #Make list of aligned permnos (ID's for each stock) of only stocks that exist within return matrix of past 12 months\n        Sigma = compute_cov_matrix(ret_matrix) #Call compute_cov_matrix by passing in all the actualized risk premia in the past 12 months for every stock\n        Sigma = pd.DataFrame(Sigma, index=aligned_permnos, columns=aligned_permnos) #Convert the covariance matrix to a dataframe with the integer permnos on both axes\n\n        current_df = df_base[df_base['month'] == month].copy() #Get this current months data from the DataFrame\n        current_df['permno'] = current_df['permno'].astype(int) #Convert permnos to integer if they are not already\n        current_df = current_df[current_df['permno'].isin(aligned_permnos)].dropna(  #Drop rows that have N/A values for any of the arbitrage difficulty interaction terms \n            subset=list(DTA_INTERACTION_MAP.keys()) + ['ret_excess', 'predictions'] #or actualized risk premia or predicted risk premia (Shouldn't be any)\n        )\n\n        print(f\"Processing {current_date.strftime('%Y-%m')}\") \n        \n        if current_df.empty: #Should never happen, SAFEGUARD\n            print(f\"------------{current_date.strftime('%Y-%m')}: No valid stocks after filtering------------\")\n            continue\n\n        current_df = current_df.set_index('permno').reindex(aligned_permnos).dropna().reset_index() #Align DataFrame with aligned_permnos and drop any N/A\n        aligned_permnos = current_df['permno'].tolist() #Redefine aligned_permnos as all the permnos in DataFrame after alignment\n        Sigma = Sigma.loc[aligned_permnos, aligned_permnos].values #Align covariance matrix with only values in aligned_permnos\n\n        mu = current_df['predictions'].values #Get risk premia OOS predictions from the current month's data\n        D = compute_di(current_df).reindex(current_df.index).fillna(0).values #Call compute_di to compute the arbitrage difficulty penalty for each permno\n\n        if w_prev.empty: #If this is the first month iteration\n            w_prev = get_initial_HL_weights(pd.Series(mu, index=aligned_permnos)) #Set previous weights to an initial H-L portfolio, similar to Gu et. al H-L portfolio\n        else: #If this is not the first month iteration\n            w_prev = w_prev.reindex(aligned_permnos).fillna(0.0) #Fill new stocks that were not present in the universe last month with 0.\n\n        w_opt = optimize_weights(mu, Sigma, D, w_prev.values, lambda_t, lambda_r, lambda_a) #Call optimize_weights() to run optimization for this month\n\n        #Now, we calculate performance metrics using the optimized portfolio weights\n        realized_return = float(np.dot(w_opt, current_df['ret_excess'].values)) #Compute actual portfolio return as the dot product of optimized weights and the actualized stock returns\n        predicted_return = float(np.dot(w_opt, mu)) #Compute predicted portfolio return as the dot product of optimized weights and predicted stock returns\n        ex_ante_risk = float(np.dot(w_opt, np.dot(Sigma, w_opt))) #Compute ex-ante risk (expected portfolio variance, anticpated volatility of future returns) based on optimized weights and the covariance matrix.\n        turnover = float(np.sum(np.abs(w_opt - w_prev.values))) #Compute portfolio turnover between optimized weights and last months optimized weights\n        arb_exposure = float(np.dot(np.abs(w_opt), D)) #Compute arbitrage difficulty with optimized weights (weights are not squared like they are in optimization objective, that is by design, we don't want this performance metric to be quadratic)\n\n        weights_history.append(w_opt) #Add the optimized weights to the weights_history array\n        returns_history.append({ #Add this month's performance metrics dictionary to the returns_history array\n            'date': current_date,\n            'return': realized_return,\n            'predicted': predicted_return,\n            'ex_ante_risk': ex_ante_risk,\n            'turnover': turnover,\n            'arb_exposure': arb_exposure\n        })        \n        \n        w_prev = pd.Series(w_opt, index=aligned_permnos) #Re-define previous weights as the current optimized weights for next iteration\n\n        print(f\"Optimized for {current_date.strftime('%Y-%m')}\") \n\n    returns_df = pd.DataFrame(returns_history)\n    returns_df = returns_df.sort_values(by='date').reset_index(drop=True) #Sort metrics DataFrame by date\n    return returns_df #Return metrics DataFrame\n\nprint(\"Optimization runner function defined\")\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "565c3354-e760-4d12-a101-4cd3ddb2eaed",
      "cell_type": "code",
      "source": "def compute_optimized_summary(returns_df): #Function to build the summary table for optimized portfolio\n    df = returns_df.copy() #Copy DataFrame into new container\n\n    df['return'] = df['return'].astype(float) #Convert returns column to float\n    df['predicted'] = pd.to_numeric(df.get('predicted'), errors='coerce') #Convert predictions column to numeric\n    df['ex_ante_risk'] = pd.to_numeric(df.get('ex_ante_risk'), errors='coerce') #Convert ex-ante risk column to numeric\n    df['turnover'] = pd.to_numeric(df.get('turnover'), errors='coerce') #Convert turnover column to numeric\n    df['arb_exposure'] = pd.to_numeric(df.get('arb_exposure'), errors='coerce') #Convert arb_exposure column to numeric\n\n    predicted_mean = df['predicted'].mean(skipna=True) * 100 #Get the percentage mean of the predicted column\n    realized_mean = df['return'].mean() * 100 #Get the percentage mean of the realized column\n    realized_sd = df['return'].std() * 100 #Compute the standard deviation column of realized returns\n    sharpe_ratio = (realized_mean / realized_sd) * np.sqrt(12) #Compute the annualized Sharpe Ratio (risk-adjusted performance metric)\n    avg_ex_ante_risk = df['ex_ante_risk'].mean(skipna=True) * 100 #Get the percentage mean of the ex-ante risk column\n    avg_turnover = df['turnover'].mean(skipna=True) * 100 #Get the percentage mean of the turnover column\n    avg_arb_exposure = df['arb_exposure'].mean(skipna=True) #Get the percentage mean of the arbitrage difficulty exposure column\n\n    summary_df = pd.DataFrame({ #Build the final summary table with all these performance metrics\n        'portfolio': ['Optimized'],\n        'predicted_mean': [predicted_mean],\n        'realized_mean': [realized_mean],\n        'realized_sd': [realized_sd],\n        'sharpe_ratio': [sharpe_ratio],\n        'avg_ex_ante_risk': [avg_ex_ante_risk],\n        'avg_turnover': [avg_turnover],\n        'avg_arb_exposure': [avg_arb_exposure]\n    })\n\n    return summary_df #Return the summary table \n\ndef get_filename(lt, lr, la): #Function to build filename for storing the results for a lambda combination\n    return f\"lambda_t{lt}_r{lr}_a{la}.pkl\" #Save in simple .pkl file\n\nprint(\"Summary function defined.\")\n",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Summary function defined.\n"
        }
      ],
      "execution_count": 18
    },
    {
      "id": "0d621370-c763-41db-9747-4b15ebe3e719",
      "cell_type": "code",
      "source": "#This cell is where we test performance for different combinations of lambda penalty values for turnover, risk, and arbitrage difficulty\nlambda_configs = [ #Array to store all the combinations of lambdas we want to try\n    (0.6, 0.5, 0.6), #Neutral penalties (pension fund-ish)\n    (0.3, 0.2, 0.3), #Liberal penalties (hedge fund-ish)\n    (1.0, 1.2, 1.0), #Conservative penalties (retail investor-ish)\n    (1.0, 0.7, 1.5), #Conservative penalties (conservative mutual fund-ish)\n]\n\n#Now we will add the cartesian product of the following possible lambdas\nlambda_t_values = [0.3, 0.6, 1.0] #Turnover lambdas we want to try\nlambda_r_values = [0.2, 0.5, 1.0] #Risk lambdas we want to try\nlambda_a_values = [0.3, 0.6, 1.0] #Arb difficulty lambdas we want to try\n\nlambda_configs = lambda_configs + list(product(lambda_t_values, lambda_r_values, lambda_a_values)) #Add the cartestian product of the possible lambdas 2 x 3 x 3 = 18 additional configs, 22 total\n\nfor i, (lt, lr, la) in enumerate(lambda_configs): #Iterate through each lambda combination'\n    filename = get_filename(lt, lr, la) #Make filename string using these lambdas\n    \n    print(f\"Running Î»_t={lt}, Î»_r={lr}, Î»_a={la}\")\n\n    #Check if file already exists\n    if os.path.exists(filename):\n        print(filename, \"already exists. Skipping.\")\n        continue\n\n    returns_df = run_portfolio_optimization(lt, lr, la, full_oos=False, test_years=[2004, 2005, 2008, 2009]) #Run portfolio optimization for this lambda combination \n    #We pick 2004, 2005, 2008, 2009 to test on a mix of smooth and rocky market conditions\n    \n    summary_df = compute_optimized_summary(returns_df) #Make summary table for this lambda combination\n    \n    result = { #Construct result dictionary containing which lambdas we're on and their corresponding summary table \n        'lambdas': {'lambda_t': lt, 'lambda_r': lr, 'lambda_a': la},\n        'summary': summary_df\n    }\n\n    with open(filename, \"wb\") as f: #Store result dictionary into a .pkl file\n        pickle.dump(result, f)\n\n    print(filename, \" was saved successfully.\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Running Î»_t=1.0, Î»_r=1.2, Î»_a=1.0\n"
        },
        {
          "ename": "NameError",
          "evalue": "name 'run_portfolio_optimization' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(year_filename, \u001b[33m\"\u001b[39m\u001b[33malready exists. Skipping.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m returns_df = \u001b[43mrun_portfolio_optimization\u001b[49m(lt, lr, la, full_oos=\u001b[38;5;28;01mFalse\u001b[39;00m, test_years=[year_winner]) \u001b[38;5;66;03m#Run portfolio optimization for this lambda combination \u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m#We pick 2004, 2005, 2008, 2009 to test on a mix of smooth and rocky market conditions\u001b[39;00m\n\u001b[32m     42\u001b[39m summary_df = compute_optimized_summary(returns_df) \u001b[38;5;66;03m#Make summary table for this lambda combination\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'run_portfolio_optimization' is not defined"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "id": "c3a40eef-5951-4712-b136-cd957975b32a",
      "cell_type": "code",
      "source": "folder_path = r\"YOUR_FOLDER_PATH_STORING_PKL_FILES\"\nfile_names = os.listdir(folder_path) #Put all .pkl files in a folder and list of all them here\n\nresults = {} #results dictionary to store results of all lambda combinations\nfor file_name in file_names: #Iterate through each .pkl filename\n    full_path = os.path.join(folder_path, file_name) #Get full path\n    with open(full_path, \"rb\") as f: #Open file\n        results[file_name] = pickle.load(f) #Add to results dictionary\n\nall_summaries = [] #Build metrics DataFrame\nfor file_name, result in results.items(): #Iterate through each lambda's value in results\n    s = result['summary'].iloc[0].copy() #Get summary section\n    s['file_name'] = file_name  #Store file name as plain string\n    all_summaries.append(s) \n\ndf_metrics = pd.DataFrame(all_summaries) #Make actual DF\n\n#For normalizing metrics\nmetric_info = {\n    'realized_mean': {'invert': False}, #Better if higher\n    'sharpe_ratio': {'invert': False}, #Better if higher\n    'avg_ex_ante_risk': {'invert': True}, #Better if lower\n    'avg_turnover': {'invert': True}, #Better if lower\n    'avg_arb_exposure': {'invert': True}, #Better if lower\n}\n\nfor metric, info in metric_info.items(): #Iterate through each metric\n    col = df_metrics[metric] #Get the column for the metric\n    normalized = (col - col.min()) / (col.max() - col.min()) #Normalize\n    if info['invert']:\n        normalized = 1 - normalized #Invert if necessary\n    df_metrics[f'norm_{metric}'] = normalized #Store\n\n#Dictionary mapping investor types to their relative priority (1â€“10) for each portfolio performance metric.\n#These weights reflect how much each investor group values metrics like realized return, Sharpe ratio,\n#ex-ante risk, turnover, and arbitrage exposure. Higher numbers indicate stronger preferences.\ninvestor_priorities = {\n    'retail_investor': {\n        'realized_mean': 3,\n        'sharpe_ratio': 5,\n        'avg_ex_ante_risk': 9,\n        'avg_turnover': 8,\n        'avg_arb_exposure': 6\n    },\n    'pension_fund': {\n        'realized_mean': 6,\n        'sharpe_ratio': 8,\n        'avg_ex_ante_risk': 6,\n        'avg_turnover': 5,\n        'avg_arb_exposure': 3\n    },\n    'mutual_fund': {\n        'realized_mean': 5,\n        'sharpe_ratio': 6,\n        'avg_ex_ante_risk': 5,\n        'avg_turnover': 5,\n        'avg_arb_exposure': 8\n    },\n    'hedge_fund': {\n        'realized_mean': 10,\n        'sharpe_ratio': 10,\n        'avg_ex_ante_risk': 2,\n        'avg_turnover': 2,\n        'avg_arb_exposure': 2\n    }\n}\n\ndef score_config(row, group): #Define scoring function\n    weights = investor_priorities[group] #Get weights for investor \n    score = 0 #Set score to 0\n    for metric in weights: #Iterate through each of this group's metric and weight\n        score += weights[metric] * row[f'norm_{metric}'] #Add weighted score for this metric to overall score\n    return score #Return final score\n\ndf_metrics.reset_index(drop=True, inplace=True)\n\nfor group in investor_priorities.keys(): #Iterate through each group\n    df_metrics[f'{group}_score'] = df_metrics.apply(lambda row: score_config(row, group), axis=1) #Get final score for each lambda combination for this group\n    \n    best_idx = df_metrics[f'{group}_score'].idxmax() #Get best score\n    best_file = df_metrics.loc[best_idx, 'file_name']  #Get filename for best score\n    \n    print(f\"\\nBest config for {group}:\") #Print results\n    print(f\"Lambdas: {results[best_file]['lambdas']}\")\n    print(f\"Summary:\\n{results[best_file]['summary']}\")\n\n#Extract lambdas from filename for clarity in the table\ndf_metrics['lambda_t'] = df_metrics['file_name'].apply(lambda x: float(x.split('_r')[0].replace('lambda_t', '').replace('.pkl', '').replace('a', '')))\ndf_metrics['lambda_r'] = df_metrics['file_name'].apply(lambda x: float(x.split('_r')[1].split('_a')[0]))\ndf_metrics['lambda_a'] = df_metrics['file_name'].apply(lambda x: float(x.split('_a')[1].replace('.pkl', '').replace('\\n', '')))\n\ncolumns_to_display = [ #Keep only relevant columns for the final output\n    'lambda_t', 'lambda_r', 'lambda_a',\n    'realized_mean', 'sharpe_ratio', 'avg_ex_ante_risk',\n    'avg_turnover', 'avg_arb_exposure'\n]\n\ndf_metrics = df_metrics.sort_values(by=['lambda_t', 'lambda_r', 'lambda_a']).reset_index(drop=True) #Sort\n\nprint(\"\\n=== Results Table: All Lambda Configurations ===\\n\")\nprint(df_metrics[columns_to_display].to_string(index=False)) #Print full table\n\n#Thank you! Please reach out with any questions at reginaldjhyde@gmail.com or hyde9698@stthomas.edu",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2c17cbbe-17d4-428d-920b-12e6e162a68c",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}