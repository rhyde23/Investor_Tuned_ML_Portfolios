{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "d3ef53f5-a464-4774-aa85-3d4f63955e2d",
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Author: Reggie Hyde\n",
        "Date: Spring 2025\n",
        "Description: This Python jupyter notebook replicates the results of\n",
        "Investor-Tuned ML Portfolios with Turnover, Risk, and Arbitrage Difficulty Controls\n",
        "The notebook is split into 2 main parts:\n",
        "1) Replicating Gu et. al's NN3 model from Empirical Asset Pricing via Machine Learning (2020)\n",
        "2) Proposing a framework for multi-objective portfolio optimization that accounts for practicality across investor types, driven by risk premia\n",
        "predictions from Gu et. al's NN3 model.\n",
        "\"\"\"\n",
        "\n",
        "#Thank you! Please reach out with any questions at reginaldjhyde@gmail.com or hyde9698@stthomas.edu\n",
        "\n",
        "#Library imports\n",
        "import os #For saving checkpoints, accessing characteristic data, etc.\n",
        "import glob #For file searching\n",
        "import numpy as np #For matrix math\n",
        "import pandas as pd #For tabular data\n",
        "import gc #For manual garbage collecting for memory management (optional, might be useful for those with limited computational power)\n",
        "import pickle #For saving data\n",
        "from itertools import product #For using cartesian product\n",
        "\n",
        "#Pytorch library imports\n",
        "import torch #For training neural network\n",
        "import torch.nn as nn #For accessing built-in common machine learning functions such as ReLU\n",
        "import torch.optim as optim #For accessing built-in common machine learning optimizers (we use Adam)\n",
        "from torch.utils.data import TensorDataset, DataLoader #For training data management\n",
        "from torch.cuda.amp import autocast, GradScaler #For accessing mixed-precision training\n",
        "\n",
        "#Scikit-learn imports\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid #For model selection functionality\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "id": "d3ef53f5-a464-4774-aa85-3d4f63955e2d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "21ad75cd-c748-4fdd-8b88-fea7588070da",
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "THE FOLLOWING CELLS REPLICATE GU ET. AL'S NN3 NEURAL NETWORK\n",
        "First, we must replicate their training, validation, and out-of-sample testing time window specifications.\n",
        "Gu. et al implement a validation rolling window of sorts.\n",
        "\"\"\"\n",
        "\n",
        "validation_length = 12 #Define the length of the validation period, in years\n",
        "\n",
        "years = np.arange(1987, 2017) #Create an array of years from 1987 to 2016 (end is non-inclusive)\n",
        "\n",
        "estimation_periods = pd.DataFrame({ #Build a DataFrame with time period specifications\n",
        "    'oos_year': years, #Out-of-sample year for testing\n",
        "    'validation_end': years-1, #Validation period ends one year before the OOS year\n",
        "    'validation_start': years - validation_length, #Validation starts 'validation_length' years before OOS year\n",
        "    'training_start': 1957, #Training period starts from 1957\n",
        "    'training_end': years - validation_length - 1 #Training ends one year before validation starts\n",
        "})\n",
        "\n",
        "print(estimation_periods.head()) #Print out the first 5 periods to verify replication"
      ],
      "metadata": {
        "scrolled": true,
        "id": "21ad75cd-c748-4fdd-8b88-fea7588070da"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fab2f84c-8636-4ae7-baf6-78e6a334ba20",
      "cell_type": "code",
      "source": [
        "class NN3(nn.Module): #This class defines the neural network model architecture for the NN3 model\n",
        "    def __init__(self, input_dim):\n",
        "        super(NN3, self).__init__()\n",
        "        self.net = nn.Sequential( #Stacks layers in following order\n",
        "            nn.Linear(input_dim, 32), #The network compresses the 920 interaction term inputs into 32 neurons in the first hidden layer\n",
        "            nn.ReLU(), #ReLU activation function\n",
        "            nn.Linear(32, 16), #The network compresses the 32 neurons in the first hidden layer into the 16 neurons in the second hidden layer\n",
        "            nn.ReLU(), #ReLU activation function\n",
        "            nn.Linear(16, 8), #The network compresses the 16 neurons in the second hidden layer into the 8 neurons in the third hidden layer\n",
        "            nn.ReLU(), #ReLU activation function\n",
        "            nn.Linear(8, 1) #The network compresses the 8 neurons in the third hidden layer into the output (predicted risk premia)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): #Specifies forward pass\n",
        "        return self.net(x)\n",
        "\n",
        "print(\"NN3 model defined.\")"
      ],
      "metadata": {
        "id": "fab2f84c-8636-4ae7-baf6-78e6a334ba20"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bec86e9a-9c3b-459e-a859-74f08106c3d5",
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, lr, weight_decay, epochs=100): #This function trains the model for a given OOS year for a given combination of hyperparameters\n",
        "    device = torch.device(\"cuda\") #Find GPU. Note: Training this model on a CPU alone for all the OOS years will be a significant undertaking.\n",
        "    model = model.to(device) #Designate GPU to train model\n",
        "\n",
        "    criterion = nn.MSELoss() #Specify loss function as Mean Squared Error\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) #Specify optimizer as Adam with current hyperparameters\n",
        "    scaler = torch.amp.GradScaler() #Specify scaler as gradient scaler for mixed-precision training\n",
        "\n",
        "    #INITIAL TRAINING\n",
        "    for epoch in range(epochs): #Iterate through n number of epochs, default will be 100\n",
        "        model.train() #Specify that we're training the model\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for xb, yb in train_loader: #Iterate through each small batch of input data and their corresponding outputs\n",
        "            xb, yb = xb.to(device), yb.to(device) #Move batch of data onto the GPU\n",
        "            optimizer.zero_grad() #Clear gradients from last backward pass\n",
        "\n",
        "            with torch.amp.autocast(\"cuda\"): #Specify automatic mixed precision to GPU\n",
        "                loss = criterion(model(xb).squeeze(), yb) #Compute training loss\n",
        "\n",
        "            scaler.scale(loss).backward() #Compute gradients for this batch\n",
        "            scaler.step(optimizer) #Update weights based on these gradients\n",
        "            scaler.update() #Dynamically adjust scaling factor\n",
        "\n",
        "            running_loss += loss.item() #Add this batch's loss to total for later average calculation\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader) #Calculate average loss across all batches\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    #VALIDATION (for evaluating best set of hyperparameters later)\n",
        "    model.eval() #Specify that we're now validating the model\n",
        "    val_losses = [] #Initialize empty array of batch validation losses\n",
        "    with torch.no_grad(): #Turn off gradient tracking\n",
        "        for xb, yb in val_loader: #Iterate through each small batch of validation data and their corresponding outputs\n",
        "            xb, yb = xb.to(device), yb.to(device) #Move batch of data onto the GPU\n",
        "\n",
        "            with torch.amp.autocast(\"cuda\"): #Specify automatic mixed precision to GPU\n",
        "                val_loss = criterion(model(xb).squeeze(), yb) #Compute validation loss\n",
        "\n",
        "            val_losses.append(val_loss.item()) #Add this batch's loss to array for later average calculation\n",
        "\n",
        "    return model, np.mean(val_losses) #Return the trained model and the validation loss average\n",
        "\n",
        "print(\"Training function defined.\")"
      ],
      "metadata": {
        "id": "bec86e9a-9c3b-459e-a859-74f08106c3d5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1ae29b35-8e85-4c98-8354-e0c2f4676b7b",
      "cell_type": "code",
      "source": [
        "param_grid = { #Define the possible values for learning rate and weight decay, based on best practices (citations required)\n",
        "    'learning_rate': [0.001, 0.01],\n",
        "    'penalty': [1e-5, 1e-3],\n",
        "}\n",
        "grids = list(ParameterGrid(param_grid)) #Define the hyperparameter grid for learning rate and weight decay\n",
        "print(\"Hyperparameter grid defined.\")"
      ],
      "metadata": {
        "id": "1ae29b35-8e85-4c98-8354-e0c2f4676b7b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "27aadf7d-0d90-41d9-93a4-7dde638cd5b9",
      "cell_type": "code",
      "source": [
        "#ACKNOWLEDGEMENT: Characteristics data was prepared with the help of a Tidy Finance blog post by Stefan Voigt (https://www.tidy-finance.org/blog/gu-kelly-xiu-replication/)\n",
        "#This cell loads all the prepared characteristic data\n",
        "downloads_folder = r\"PATH_OF_YOUR_PREPARED_CHARACTERISTICS\" #Specify path of downloads folder\n",
        "file_paths = glob.glob(os.path.join(downloads_folder, \"characteristics_prepared_*/year=*/part-0.parquet\")) #Locate all .parquet files by year. (This file path format is how the Tidy Finance blog post does it.)\n",
        "all_data = {}  #Initialize an empty dictionary to store data keyed by year\n",
        "\n",
        "for path in file_paths_for_neural_net_replication :  #Iterate over each file path of characteristics data by year\n",
        "    chunk = pd.read_parquet(path)  #Read the parquet file into a pandas DataFrame\n",
        "    year = int(path.split('=')[-1].split('\\\\')[0])  #Extract the year from the file path\n",
        "    all_data[year] = chunk  #Store the year's data in the dictionary with the corresponding year as the key\n",
        "    print(f\"Loaded data for year: {year}\")\n",
        "\n",
        "characteristics_prepared_all = pd.concat(all_data.values(), ignore_index=True) #Concatenate all yearly DataFrames into one DataFrame\n",
        "characteristics_prepared_all['year'] = pd.to_datetime(characteristics_prepared_all['date'], errors='coerce').dt.year #Create column in DataFrame for just year\n",
        "\n",
        "gc.collect() #Manually free up memory"
      ],
      "metadata": {
        "id": "27aadf7d-0d90-41d9-93a4-7dde638cd5b9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d065a715-0b69-4cb7-ae84-d48612ca68a8",
      "cell_type": "code",
      "source": [
        "#This cell trains the model for a given j OOS year.\n",
        "#For the purposes of strictly replicating Gu et. al, full window is only range(0, 30, 1)\n",
        "\n",
        "neural_net_replication_range = list(range(0, 30, 1)) #Range for replicating neural network from Gu. et al\n",
        "\n",
        "for j in neural_net_replication_range: #Iterate through each OOS year. 30 years = to 2016\n",
        "    print(f\"\\nProcessing iteration: {j+1}\")\n",
        "\n",
        "    split_dates = estimation_periods.iloc[j] #Get split dates for this OOS years\n",
        "    cutoff_year = split_dates['validation_end'] #Get end year of validation window\n",
        "    data = characteristics_prepared_all[characteristics_prepared_all['year'] <= cutoff_year] #Get data for training + validation\n",
        "    train = data[data['year'] < split_dates['training_end']] #Define training set\n",
        "    validation = data[(data['year'] >= split_dates['validation_start']) &  (data['year'] <= split_dates['validation_end'])] #Define validation set\n",
        "\n",
        "    columns_to_drop = ['permno', 'date', 'mktcap_lag', 'year'] #Define list of columns to drop from predictors\n",
        "    X_train = train.drop(columns=columns_to_drop + ['ret_excess']) #Prepare training inputs\n",
        "    y_train = train['ret_excess'] #Prepare training outputs (ret_excess column contains risk premia)\n",
        "\n",
        "    X_val = validation.drop(columns=columns_to_drop + ['ret_excess']) #Prepare validation inputs\n",
        "    y_val = validation['ret_excess'] #Prepare validation outputs\n",
        "\n",
        "    X_train = X_train.fillna(X_train.mean()) #Fill any N/A values in training inputs with the mean of its column\n",
        "    X_val = X_val.fillna(X_val.mean()) #Fill any N/A values in training inputs with the mean of its column\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32) #Convert training inputs into PyTorch tensor\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32) #Convert training output into PyTorch tensor\n",
        "    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32) #Convert validation inputs into PyTorch tensor\n",
        "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32) #Convert validation output into PyTorch tensor\n",
        "\n",
        "    train_ds = TensorDataset(X_train_tensor, y_train_tensor) #Create TensorDataaset for training data (inputs, output)\n",
        "    val_ds = TensorDataset(X_val_tensor, y_val_tensor) #Create TensorDataaset for validation data (inputs, output)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=10000, shuffle=True) #Create DataLoader for training data. Batch size is adjustable based on computational power/memory\n",
        "    val_loader = DataLoader(val_ds, batch_size=10000) #Create DataLoader for validation data. Batch size is adjustable based on computational power/memory\n",
        "\n",
        "    best_loss = float('inf') #Initialize best loss float\n",
        "    best_model = None #Initialize best model container\n",
        "    best_params = None #Initialize best model parameters container\n",
        "\n",
        "    for params in grids: #Loop over all hyperparameter combinations in the grid\n",
        "        model = NN3(input_dim=X_train.shape[1]) #Create model object using NN3 class\n",
        "        trained_model, val_loss = train_model(model, train_loader, val_loader, lr=params['learning_rate'], weight_decay=params['penalty']) #Call \"train_model()\"\n",
        "        if val_loss < best_loss: #If this train had a superior loss, save model as best model so far\n",
        "            best_loss = val_loss\n",
        "            best_model = trained_model\n",
        "            best_params = params\n",
        "\n",
        "    model_path = f\"NN3_fitted_model_{split_dates['oos_year']}.pt\" #Define file path for saving best model\n",
        "    torch.save(best_model.state_dict(), model_path) #Save the best model for this OOS year to this file\n",
        "    print(f\"Saved model to {model_path}\")\n"
      ],
      "metadata": {
        "id": "d065a715-0b69-4cb7-ae84-d48612ca68a8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cff47081-f575-4ff5-b91d-aa499b4af4b4",
      "cell_type": "code",
      "source": [
        "#This cell makes OOS predictions using trained models for each OOS year\n",
        "#Then, portfolio is constructed by going long on the top 10 percent of stocks by predicted risk premia and going short on the bottom 10 percent\n",
        "#This is called the H-L portfolio. The table printed out at the bottom replicates Gu et. al methodology\n",
        "\n",
        "neural_net_replication_oos_range = list(range(1987, 2017)) #1987-2016, OOS range for replicating Table 7 in Gu et. al paper\n",
        "oos_predictions = [] #Initialize empty array for OOS predictions\n",
        "\n",
        "for oos_year in range(1986, 2017):  #Iterate through each OOS year\n",
        "    print(f\"Processing OOS year: {oos_year}\")\n",
        "\n",
        "    feature_cols = [col for col in characteristics_prepared_all.columns if col not in ['permno', 'date', 'mktcap_lag', 'ret_excess', 'year']] #Get all predictor columns\n",
        "\n",
        "    base_dir = r\"YOUR_BASE_DIRECTORY_STORING_FITTED_MODELS\"\n",
        "    model_path = rf\"{base_dir}\\NN3_fitted_model_{oos_year}.pt\" #Load the saved fitted model for this OOS year\n",
        "    model = NN3(input_dim=len(feature_cols)) #Create model object using NN3 class\n",
        "    model.load_state_dict(torch.load(model_path)) #Load model\n",
        "    model.eval() #Specify model to be in eval mode, which is for validation or testing the model with out-of-sample data\n",
        "\n",
        "    oos_data = characteristics_prepared_all[characteristics_prepared_all['year'] == oos_year].copy() #Get characteristics data from this OOS year\n",
        "    oos_data.dropna(subset=['ret_excess'], inplace=True) #Drop any N/A values in the OOS data (shouldn't be any, but procedural assurance)\n",
        "\n",
        "    X_oos = oos_data[feature_cols] #Get OOS input data in the feature columns\n",
        "    X_oos = X_oos.fillna(X_oos.mean()) #Fill any N/A values in OOS inputs with the mean of its column\n",
        "    X_oos_tensor = torch.tensor(X_oos.values, dtype=torch.float32) #Convert OOS input data to Tensor\n",
        "\n",
        "    with torch.no_grad(): #Specify gradient tracking is not necessary\n",
        "        preds = model(X_oos_tensor).squeeze() #Make model predictions\n",
        "\n",
        "    oos_chunk = oos_data[['permno', 'date', 'mktcap_lag', 'ret_excess']].copy() #Get chunk of non-feature column data for this OOS year\n",
        "    oos_chunk['predictions'] = preds.numpy() #Add the predictions column to this chunk for this OOS year\n",
        "    oos_predictions.append(oos_chunk) #Add this year's chunk to the oos_predictions array\n",
        "\n",
        "oos_predictions = pd.concat(oos_predictions, ignore_index=True) #Concatenate all year's data into one DataFrame\n",
        "\n",
        "#Now, we will construct portfolios to replicate Gu et. al\n",
        "def assign_portfolio_by_month(df, sorting_variable, n_portfolios=10): #This function will sort the DataFrame into 10 deciles based on a sorting variable, which in our case will be predicted risk premia\n",
        "    df['portfolio'] = df.groupby('date')[sorting_variable].transform(lambda x: pd.qcut(x, q=n_portfolios, labels=False, duplicates='drop') + 1)\n",
        "    return df\n",
        "\n",
        "#Sort stocks into decile portfolios based on raw predicted returns (no value-weighting at this stage)\n",
        "ml_portfolios = oos_predictions.copy() #Copy OOS predictions data\n",
        "ml_portfolios = assign_portfolio_by_month(ml_portfolios, sorting_variable='predictions', n_portfolios=10) #Construct portfolio decile groups using assign_portfolio_by_month() function, sorting variable is predicted risk premia\n",
        "ml_portfolios['portfolio'] = ml_portfolios['portfolio'].astype('category') #Convert numerical portfolio decile column into categorical variable\n",
        "ml_portfolios.dropna(subset=['ret_excess'], inplace=True) #Removes rows where ret_excess has NaN values\n",
        "\n",
        "def weighted_mean(df, value, weight): #This function calculates a weighted mean based off of a value and a corresponding weight.\n",
        "    return (df[value] * df[weight]).sum() / df[weight].sum() #In our case, value = predicted risk premia (in percentage), weight = market capitalization\n",
        "    #(to calculate realistic returns, we care more about high percentage gains/losses in bigger companies)\n",
        "\n",
        "# Value-weighted monthly returns per decile\n",
        "ml_portfolios_summary = ( #Calculate value-weighted monthly actual realized average returns and value-weighted predicted average returns and add columns to table\n",
        "    ml_portfolios.groupby(['portfolio', 'date'], observed=False)\n",
        "    .apply(lambda x: pd.Series({\n",
        "        'predictions': weighted_mean(x, 'predictions', 'mktcap_lag'), #Add predicted average returns column (value-weighted)\n",
        "        'ret_excess': weighted_mean(x, 'ret_excess', 'mktcap_lag') #Add realized average returns column (value-weighted)\n",
        "    }))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "#This code constructs the H-L portfolio as described in Gu et. al (long on top 10 percent predicted, short on bottom 10 percent predicted)\n",
        "\n",
        "p10 = ml_portfolios_summary[ml_portfolios_summary['portfolio'] == 10].copy() #Copy the 10th decile (top decile in terms of raw percentage predicted risk premia)\n",
        "p1 = ml_portfolios_summary[ml_portfolios_summary['portfolio'] == 1].copy() #Copy the 1st decile (bottom decile in terms of raw percentage predicted risk premia)\n",
        "\n",
        "hl_merge = pd.merge(p10, p1, on='date', suffixes=('_10', '_1')) #Create DF by merging 10th decile and 1st decile\n",
        "hl_merge['predictions'] = hl_merge['predictions_10'] - hl_merge['predictions_1'] #Long on 10th decile stocks, short on 1st decile stocks to measure theoretical value-weighted portfolio return (for predicted risk premia)\n",
        "hl_merge['ret_excess'] = hl_merge['ret_excess_10'] - hl_merge['ret_excess_1'] #Long on 10th decile stocks, short on 1st decile stocks to measure theoretical value-weighted portfolio return (for realized risk premia)\n",
        "hml_portfolio = hl_merge[['date', 'predictions', 'ret_excess']].copy() #Add date, value-weighted predictions, and value-weighted realized returns columns to H-L\n",
        "hml_portfolio['portfolio'] = 'H-L' #Name: H-L\n",
        "\n",
        "ml_portfolios_summary_updated = pd.concat([ml_portfolios_summary, hml_portfolio], ignore_index=True) #Combine 10 deciles with H-L portfolio in the table\n",
        "\n",
        "overall_summary = ml_portfolios_summary_updated.groupby('portfolio').agg( #Construct overall summary table\n",
        "    Pred=('predictions', 'mean'),\n",
        "    Avg=('ret_excess', 'mean'),\n",
        "    Std=('ret_excess', 'std')\n",
        ").reset_index()\n",
        "\n",
        "overall_summary['Pred'] *= 100 #Convert predicted risk premia to percentage format\n",
        "overall_summary['Avg'] *= 100 #Convert realized risk premia to percentage format\n",
        "overall_summary['Std'] *= 100 #Convert standard deviation of realized risk premia to percentage format\n",
        "overall_summary['SR'] = ( #Calculate annualized Sharpe ratio\n",
        "    overall_summary['Avg'] / overall_summary['Std']\n",
        ") * (12 ** 0.5)\n",
        "\n",
        "#Display table\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(overall_summary)\n"
      ],
      "metadata": {
        "id": "cff47081-f575-4ff5-b91d-aa499b4af4b4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b061f727-bc9a-4e5b-966e-bb9961582d60",
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "THE FOLLOWING CELLS ARE THE CONTRIBUTION OF THIS PAPER\n",
        "The following cells propose a portfolio optimization model, which aims to solve a customizable objective function using Sequential Least Squares Programming (SLSQP)\n",
        "\"\"\"\n",
        "from datetime import datetime #For dealing with dates\n",
        "from scipy.optimize import minimize #For using minimize function, which is a general-purpose nonlinear solver\n",
        "from sklearn.preprocessing import StandardScaler #For calculating Z-scored arbitrage difficulty penalty\n",
        "from dateutil.relativedelta import relativedelta #For date arithmetic\n",
        "import time #For dealing with time\n",
        "\n",
        "OOS_OPT_START_YEAR = 1987 #Optimization year range begins here\n",
        "\n",
        "CHARACTERISTIC_IMPORTANCE = { #Define importance weight for each stock-level DTA score interaction term (These importance weights were determined from analysis of existing literature)\n",
        "    \"ill\": 1.00,\n",
        "    \"baspread\": 1.00,\n",
        "    \"idiovol\": 0.70,\n",
        "    \"turn\": 0.50,\n",
        "}\n",
        "\n",
        "MACRO_IMPORTANCE = { #Define importance weight for each macroeconomic DTA score interaction term (These importance weights were determined from analysis of existing literature)\n",
        "    \"svar\": 1.00,\n",
        "    \"tbl\": 0.70\n",
        "}\n",
        "\n",
        "#Cartesian product of stock-level characteristics: (ill, idiovol, baspread, turn) and macroeconomic characteristics: (svar, tbl)\n",
        "DTA_INTERACTION_MAP = { #Map characteristics with their two components\n",
        "    \"characteristic_ill_x_macro_svar\": (\"ill\", \"svar\"),\n",
        "    \"characteristic_ill_x_macro_tbl\": (\"ill\", \"tbl\"),\n",
        "    \"characteristic_idiovol_x_macro_svar\": (\"idiovol\", \"svar\"),\n",
        "    \"characteristic_idiovol_x_macro_tbl\": (\"idiovol\", \"tbl\"),\n",
        "    \"characteristic_baspread_x_macro_svar\": (\"baspread\", \"svar\"),\n",
        "    \"characteristic_baspread_x_macro_tbl\": (\"baspread\", \"tbl\"),\n",
        "    \"characteristic_turn_x_macro_svar\": (\"turn\", \"svar\"),\n",
        "    \"characteristic_turn_x_macro_tbl\": (\"turn\", \"tbl\"),\n",
        "}\n",
        "\n",
        "DTA_WEIGHTS = {} #Initialize empty dictionary for DTA_WEIGHTS\n",
        "\n",
        "for interaction_term, (characteristic, macro) in DTA_INTERACTION_MAP.items(): #Iterate through each interaction term\n",
        "    weight = CHARACTERISTIC_IMPORTANCE[characteristic] * MACRO_IMPORTANCE[macro] #Calculate weight of each interaction term\n",
        "    DTA_WEIGHTS[interaction_term] = weight #Assign weight to interaction term\n",
        "\n",
        "WINDOW_MONTHS = 12 #Define trailing window length for covariance matrix calculating (look 1 year back)\n",
        "EPSILON = 1e-6 #Define smoothing constant for turnover penalty\n",
        "\n",
        "print(\"Optimization Initialization completed.\")"
      ],
      "metadata": {
        "id": "b061f727-bc9a-4e5b-966e-bb9961582d60"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1717dcc3-3ace-4318-9196-6e45f6126d81",
      "cell_type": "code",
      "source": [
        "def compute_di(df): #Function for computing the arbitrage difficulty penalty in the optimization problem at a given t\n",
        "    Z = df[list(DTA_WEIGHTS.keys())].copy() #Extract the columns of the interaction terms involved in calculating the arbitrage difficulty penalty\n",
        "\n",
        "    Z = Z.dropna() #Drop any rows with N/A values in these columns\n",
        "    Z_scaled = pd.DataFrame( #Standardize each interaction term to ensure fair weighting across terms\n",
        "        StandardScaler().fit_transform(Z), #Applies (x - mean) / std to each column\n",
        "        index=Z.index,\n",
        "        columns=Z.columns\n",
        "    )\n",
        "\n",
        "    weight_vector = np.array([DTA_WEIGHTS[col] for col in Z_scaled.columns]) #Get a weight vector aligned by columns/keys\n",
        "    S = Z_scaled.values @ weight_vector #Calculate row-wise dot product\n",
        "    D = (S - S.min()) / (S.max() - S.min()) #Normalize S to [0, 1]\n",
        "    return pd.Series(D, index=Z_scaled.index) #Return final D_i vector\n",
        "\n",
        "print(\"Arbitrage difficulty function defined.\")"
      ],
      "metadata": {
        "id": "1717dcc3-3ace-4318-9196-6e45f6126d81"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c5392a83",
      "cell_type": "code",
      "source": [
        "def compute_cov_matrix(returns_df): #Function to calculate the covariance matrix of the stocks at a given t\n",
        "    return np.cov(returns_df.T) #Return covariance matrix (transpose returns so that np.cov treats columns as each stock)\n",
        "\n",
        "print(\"Covariance matrix function defined.\")"
      ],
      "metadata": {
        "id": "c5392a83"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "20286676",
      "cell_type": "code",
      "source": [
        "def optimization_objective(w, mu, Sigma, D, w_prev, lambda_t, lambda_r, lambda_a): #This function defines the optimization objective expression to be minimized\n",
        "    #w = current weight vector (currently being optimized), mu = predicted risk premia output from model, Sigma = covariance matrix, D = computed arbitrage difficulty score [0, 1], w_prev: previous month's weights,\n",
        "    #lambda_t: penalty weight for turnover, lambda_r: Penalty weight for risk, lambda_a: Penalty weight for arbitrage difficulty\n",
        "\n",
        "    turnover = np.sum(np.sqrt((w - w_prev)**2 + EPSILON)) #Define smoothed turnover penalty that approximates |w_i - w_prev_i|.\n",
        "    #Smoothed to insure differentiability (absolute value functions are indifferentiable at x=0, which is non-friendly to optimzation solvers)\n",
        "\n",
        "    risk = w @ Sigma @ w #Define risk term as the transpose of the current weights multiplied by the covariance matrix, multiplied by the current weights\n",
        "    #In this case, the weights vector in the leftmost operand are implicitly transposed.\n",
        "\n",
        "    arb = np.sum(D * w**2) #Define weighted quadratic term discouraging heavy allocation to difficult-to-arbitrage stocks\n",
        "\n",
        "    percentage_returns = w @ mu #Calculate the hypothetical percentage returns with the current set of weights\n",
        "    return -(percentage_returns - lambda_t * turnover - lambda_r * risk - lambda_a * arb) #Return negative of the reward, since we use a minimizer\n",
        "\n",
        "print(\"Optimization objective function defined.\")"
      ],
      "metadata": {
        "id": "20286676"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e1e3f184-001f-486b-9caa-844755b84db2",
      "cell_type": "code",
      "source": [
        "def optimize_weights(mu, Sigma, D, w_prev, lambda_t, lambda_r, lambda_a, bounds=None): #Function to optimize the portfolio weights for one month (t)\n",
        "    #mu = predicted risk premia output from model, Sigma = covariance matrix, D = computed arbitrage difficulty score [0, 1], w_prev: previous month's weights,\n",
        "    #lambda_t: penalty weight for turnover, lambda_r: Penalty weight for risk, lambda_a: Penalty weight for arbitrage difficulty, bounds = Optional box constraints for each weight (could add some bounds to customize optimization)\n",
        "\n",
        "    n = len(mu) #Get the current number of stocks we're considering\n",
        "    w0 = np.zeros(n) #Initialize weights vector with all zeros\n",
        "    constraints = ({ #Add constraint that Weights must sum to zero\n",
        "        'type': 'eq',\n",
        "        'fun': lambda w: np.sum(w)\n",
        "    })\n",
        "\n",
        "    def optimization_callback(w): #Function to be called at each iteration of SLSQP that displays the mean, max, and min of the current weight vector\n",
        "        print(f\"   ↳ Iteration: mean(w)={w.mean():.4e}, max={w.max():.4f}, min={w.min():.4f}\")\n",
        "\n",
        "    #This is where we start the optimization function\n",
        "    print(\"Starting minimize()...\")\n",
        "    start_time = time.time() #Record start time of function\n",
        "\n",
        "    res = minimize( #Call minimize() from scipy.optimize with the following arguments:\n",
        "        optimization_objective, #Pass in the optimization objective function\n",
        "        w0, #Pass in initial w0 of all zeros\n",
        "        args=(mu, Sigma, D, w_prev, lambda_t, lambda_r, lambda_a), #Pass in tuple of additional arguments that are passed to the optimization objective, after w0.\n",
        "        constraints=constraints, #Pass in constraints\n",
        "        bounds=bounds, #Pass in bounds\n",
        "        method='SLSQP', #Specify that we want to use SLSQP\n",
        "        callback=optimization_callback, #Pass in callback function that will print out a small report after each iteration of optimization\n",
        "        options={ #Pass in a set of options\n",
        "            'disp': True, #We want final summary after convergence\n",
        "            'maxiter': 100, #Must be large number such as 100 for meaningful results (could benefit from increasing, potentially.)\n",
        "            'ftol': 1e-5 #Change of function value must be less than 10^-5 for us to stop function early\n",
        "        }\n",
        "    )\n",
        "\n",
        "    elapsed = time.time() - start_time #Calculate elapsed time from minimize() call\n",
        "    print(f\"Optimization finished in {elapsed:.3f} seconds | Success: {res.success} | Iterations: {res.nit}\\n\") #Display elapsed seconds, success, and number of iterations\n",
        "    if res.success : #If optimization problem succeeded\n",
        "        return res.x #Return optimized weights\n",
        "    else :\n",
        "        print(\"------------FAILED: RETURNED ALL-ZERO WEIGHTS MATRIX DUE TO FAILURE TO OPTIMIZE------------\") #Print failure message\n",
        "        return np.zeros(n) #Return all-zero weights matrix\n",
        "\n",
        "print(\"Optimization function defined.\")"
      ],
      "metadata": {
        "id": "e1e3f184-001f-486b-9caa-844755b84db2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "df1fedb6-8f70-47e0-bae9-9a01d4102ec5",
      "cell_type": "code",
      "source": [
        "def get_initial_HL_weights(mu_series): #Function to construct the initial w_prev vector using the H-L portfolio present in Gu et. al\n",
        "    #mu_series = series object that contains permno as the ID's for predicted risk premia (series is similar to a dictionary)\n",
        "    n = len(mu_series) #Get number of stocks currently working with\n",
        "    n_in_decile = int(n * 0.1) #Get number of stocks in a decile\n",
        "\n",
        "    mu_sorted = mu_series.sort_values(ascending=False) #Sort mu_series in descending order by its values, in this case predicted risk premia\n",
        "    top_permnos = mu_sorted.iloc[:n_in_decile].index #Store top 10 percent of stocks in terms of predicted risk premia in top_permnos\n",
        "    bottom_permnos = mu_sorted.iloc[-n_in_decile:].index #Store bottom 10 percent of stocks in terms of predicted risk premia in top_permnos\n",
        "\n",
        "    long_weight = 1.0 / n_in_decile #Equal weight long position\n",
        "    short_weight = -1.0 / n_in_decile #Equal weight short position\n",
        "\n",
        "    weights = pd.Series(0.0, index=mu_series.index) #Initialize weights vector with all zeros\n",
        "    weights.loc[top_permnos] = long_weight #Assign long_weight to top 10 percent\n",
        "    weights.loc[bottom_permnos] = short_weight #Assign short_weight to bottom 10 percent\n",
        "\n",
        "    assert np.isclose(weights.sum(), 0.0), \"Weights do not sum to zero\" #Ensure the sum of the initial weights is very close to 0\n",
        "\n",
        "    return weights #Return the H-L portfolio weights\n",
        "\n",
        "print(\"Initial H-L portfolio function defined.\")"
      ],
      "metadata": {
        "id": "df1fedb6-8f70-47e0-bae9-9a01d4102ec5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "65f97bd8",
      "cell_type": "code",
      "source": [
        "#This cell prepares for the DataFrame for the portfolio optimization function\n",
        "df_base = characteristics_prepared_all.copy() #Define DataFrame for the optimization problem\n",
        "df_base['date'] = pd.to_datetime(df_base['date']) #Convert date column in DF to datetime objects\n",
        "oos_predictions['date'] = pd.to_datetime(oos_predictions['date']) #Convert date column in oos_predictions to datetime objects\n",
        "\n",
        "df_base = pd.merge( #Merge DF with oos_predictions\n",
        "    df_base,\n",
        "    oos_predictions[['permno', 'date', 'predictions']], #We care about these 3 columns in the oos_predictions DF: permno (stock ID), date, and risk premia prediction\n",
        "    on=['permno', 'date'], #Merge on ID and date\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "df_base = df_base[['permno', 'date', 'ret_excess'] + list(DTA_INTERACTION_MAP.keys()) + ['predictions']] #Filter DataFrame to only include permno, date, ret_excess, the arbitrage difficulty\n",
        "df_base['month'] = df_base['date'].dt.to_period('M') #Add month column to DataFrame\n",
        "\n",
        "print(\"DataFrame for portfolio optimization constructed.\")"
      ],
      "metadata": {
        "id": "65f97bd8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f3b87a0e-789c-434d-a5b3-9a1f24b1fa9b",
      "cell_type": "code",
      "source": [
        "def run_portfolio_optimization(lambda_t, lambda_r, lambda_a, full_oos=True, test_years=None): #Function for running portfolio optimization for a set of lambda penalty terms\n",
        "    #lambda_t: penalty weight for turnover, lambda_r: Penalty weight for risk, lambda_a: Penalty weight for arbitrage difficulty\n",
        "    #full_oos: boolean that tells function if we are running through whole OOS window (1987-2016)\n",
        "    #test_years: if we are not running through the whole OOS window, this list specicies which years are to be tested\n",
        "\n",
        "    monthly_dates = sorted(df_base['month'].unique()) #Get all unique monthly dates in whole DataFrame\n",
        "\n",
        "    start_month = pd.Period(f'{OOS_OPT_START_YEAR}-01', freq='M') #Make start_month pd Period object based on the optimization start year\n",
        "\n",
        "    if full_oos: #If caller wants to execute for the whole OOS period\n",
        "        opt_months = [m for m in monthly_dates if m >= start_month] #Define opt_months as any month later or equal to start_month\n",
        "    else: #If not\n",
        "        opt_months = [m for m in monthly_dates if m.year in test_years] #Define opt_months as any month in the set of test_years\n",
        "\n",
        "    print(\"Number of months to optimize: \", len(opt_months))\n",
        "    print(\"First month: \", opt_months[0])\n",
        "    print(\"Last month: \", opt_months[-1])\n",
        "\n",
        "    weights_history = [] #Initialize weights_history to record the progression of weights for potential later analysis\n",
        "    returns_history = [] #Initialize returns_history to record performance of optimized portfolio\n",
        "\n",
        "    w_prev = pd.Series(dtype=float) #Initialize previous weights Series object\n",
        "\n",
        "    for month in opt_months: #Iterate through each month in opt_months\n",
        "        current_date = pd.Timestamp(month.to_timestamp()) #Get current date in months in timestamp format\n",
        "        one_year_ago = pd.Timestamp((month - WINDOW_MONTHS).to_timestamp()) #Calculate the month one year ago in timestamp format\n",
        "\n",
        "        past_returns = df_base[(df_base['month'] >= one_year_ago.to_period('M')) & (df_base['month'] < month)] #Filter DataFrame to include data between one year ago and now\n",
        "        ret_matrix = past_returns.pivot_table(index='month', columns='permno', values='ret_excess', aggfunc='mean').dropna(axis=1) #Get matrix of past realized risk premia between one year ago and now\n",
        "\n",
        "        if ret_matrix.shape[1] < 10 or ret_matrix.shape[0] < 12: #If there is not enough data for covariance matrix (SAFEGUARD, THIS SHOULD NEVER BE TRUE)\n",
        "            print(f\"------------Skipping {current_date.strftime('%Y-%m')} — insufficient data------------\")\n",
        "            continue\n",
        "\n",
        "        aligned_permnos = [int(p) for p in ret_matrix.columns.tolist()] #Make list of aligned permnos (ID's for each stock) of only stocks that exist within return matrix of past 12 months\n",
        "        Sigma = compute_cov_matrix(ret_matrix) #Call compute_cov_matrix by passing in all the actualized risk premia in the past 12 months for every stock\n",
        "        Sigma = pd.DataFrame(Sigma, index=aligned_permnos, columns=aligned_permnos) #Convert the covariance matrix to a dataframe with the integer permnos on both axes\n",
        "\n",
        "        current_df = df_base[df_base['month'] == month].copy() #Get this current months data from the DataFrame\n",
        "        current_df['permno'] = current_df['permno'].astype(int) #Convert permnos to integer if they are not already\n",
        "        current_df = current_df[current_df['permno'].isin(aligned_permnos)].dropna(  #Drop rows that have N/A values for any of the arbitrage difficulty interaction terms\n",
        "            subset=list(DTA_INTERACTION_MAP.keys()) + ['ret_excess', 'predictions'] #or actualized risk premia or predicted risk premia (Shouldn't be any)\n",
        "        )\n",
        "\n",
        "        print(f\"Processing {current_date.strftime('%Y-%m')}\")\n",
        "\n",
        "        if current_df.empty: #Should never happen, SAFEGUARD\n",
        "            print(f\"------------{current_date.strftime('%Y-%m')}: No valid stocks after filtering------------\")\n",
        "            continue\n",
        "\n",
        "        current_df = current_df.set_index('permno').reindex(aligned_permnos).dropna().reset_index() #Align DataFrame with aligned_permnos and drop any N/A\n",
        "        aligned_permnos = current_df['permno'].tolist() #Redefine aligned_permnos as all the permnos in DataFrame after alignment\n",
        "        Sigma = Sigma.loc[aligned_permnos, aligned_permnos].values #Align covariance matrix with only values in aligned_permnos\n",
        "\n",
        "        mu = current_df['predictions'].values #Get risk premia OOS predictions from the current month's data\n",
        "        D = compute_di(current_df).reindex(current_df.index).fillna(0).values #Call compute_di to compute the arbitrage difficulty penalty for each permno\n",
        "\n",
        "        if w_prev.empty: #If this is the first month iteration\n",
        "            w_prev = get_initial_HL_weights(pd.Series(mu, index=aligned_permnos)) #Set previous weights to an initial H-L portfolio, similar to Gu et. al H-L portfolio\n",
        "        else: #If this is not the first month iteration\n",
        "            w_prev = w_prev.reindex(aligned_permnos).fillna(0.0) #Fill new stocks that were not present in the universe last month with 0.\n",
        "\n",
        "        w_opt = optimize_weights(mu, Sigma, D, w_prev.values, lambda_t, lambda_r, lambda_a) #Call optimize_weights() to run optimization for this month\n",
        "\n",
        "        #Now, we calculate performance metrics using the optimized portfolio weights\n",
        "        realized_return = float(np.dot(w_opt, current_df['ret_excess'].values)) #Compute actual portfolio return as the dot product of optimized weights and the actualized stock returns\n",
        "        predicted_return = float(np.dot(w_opt, mu)) #Compute predicted portfolio return as the dot product of optimized weights and predicted stock returns\n",
        "        ex_ante_risk = float(np.dot(w_opt, np.dot(Sigma, w_opt))) #Compute ex-ante risk (expected portfolio variance, anticpated volatility of future returns) based on optimized weights and the covariance matrix.\n",
        "        turnover = float(np.sum(np.abs(w_opt - w_prev.values))) #Compute portfolio turnover between optimized weights and last months optimized weights\n",
        "        arb_exposure = float(np.dot(np.abs(w_opt), D)) #Compute arbitrage difficulty with optimized weights (weights are not squared like they are in optimization objective, that is by design, we don't want this performance metric to be quadratic)\n",
        "\n",
        "        weights_history.append(w_opt) #Add the optimized weights to the weights_history array\n",
        "        returns_history.append({ #Add this month's performance metrics dictionary to the returns_history array\n",
        "            'date': current_date,\n",
        "            'return': realized_return,\n",
        "            'predicted': predicted_return,\n",
        "            'ex_ante_risk': ex_ante_risk,\n",
        "            'turnover': turnover,\n",
        "            'arb_exposure': arb_exposure\n",
        "        })\n",
        "\n",
        "        w_prev = pd.Series(w_opt, index=aligned_permnos) #Re-define previous weights as the current optimized weights for next iteration\n",
        "\n",
        "        print(f\"Optimized for {current_date.strftime('%Y-%m')}\")\n",
        "\n",
        "    returns_df = pd.DataFrame(returns_history)\n",
        "    returns_df = returns_df.sort_values(by='date').reset_index(drop=True) #Sort metrics DataFrame by date\n",
        "    return returns_df #Return metrics DataFrame\n",
        "\n",
        "print(\"Optimization runner function defined\")\n"
      ],
      "metadata": {
        "id": "f3b87a0e-789c-434d-a5b3-9a1f24b1fa9b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "565c3354-e760-4d12-a101-4cd3ddb2eaed",
      "cell_type": "code",
      "source": [
        "def compute_optimized_summary(returns_df): #Function to build the summary table for optimized portfolio\n",
        "    df = returns_df.copy() #Copy DataFrame into new container\n",
        "\n",
        "    df['return'] = df['return'].astype(float) #Convert returns column to float\n",
        "    df['predicted'] = pd.to_numeric(df.get('predicted'), errors='coerce') #Convert predictions column to numeric\n",
        "    df['ex_ante_risk'] = pd.to_numeric(df.get('ex_ante_risk'), errors='coerce') #Convert ex-ante risk column to numeric\n",
        "    df['turnover'] = pd.to_numeric(df.get('turnover'), errors='coerce') #Convert turnover column to numeric\n",
        "    df['arb_exposure'] = pd.to_numeric(df.get('arb_exposure'), errors='coerce') #Convert arb_exposure column to numeric\n",
        "\n",
        "    predicted_mean = df['predicted'].mean(skipna=True) * 100 #Get the percentage mean of the predicted column\n",
        "    realized_mean = df['return'].mean() * 100 #Get the percentage mean of the realized column\n",
        "    realized_sd = df['return'].std() * 100 #Compute the standard deviation column of realized returns\n",
        "    sharpe_ratio = (realized_mean / realized_sd) * np.sqrt(12) #Compute the annualized Sharpe Ratio (risk-adjusted performance metric)\n",
        "    avg_ex_ante_risk = df['ex_ante_risk'].mean(skipna=True) * 100 #Get the percentage mean of the ex-ante risk column\n",
        "    avg_turnover = df['turnover'].mean(skipna=True) * 100 #Get the percentage mean of the turnover column\n",
        "    avg_arb_exposure = df['arb_exposure'].mean(skipna=True) #Get the percentage mean of the arbitrage difficulty exposure column\n",
        "\n",
        "    summary_df = pd.DataFrame({ #Build the final summary table with all these performance metrics\n",
        "        'portfolio': ['Optimized'],\n",
        "        'predicted_mean': [predicted_mean],\n",
        "        'realized_mean': [realized_mean],\n",
        "        'realized_sd': [realized_sd],\n",
        "        'sharpe_ratio': [sharpe_ratio],\n",
        "        'avg_ex_ante_risk': [avg_ex_ante_risk],\n",
        "        'avg_turnover': [avg_turnover],\n",
        "        'avg_arb_exposure': [avg_arb_exposure]\n",
        "    })\n",
        "\n",
        "    return summary_df #Return the summary table\n",
        "\n",
        "def get_filename(lt, lr, la): #Function to build filename for storing the results for a lambda combination\n",
        "    return f\"lambda_t{lt}_r{lr}_a{la}.pkl\" #Save in simple .pkl file\n",
        "\n",
        "print(\"Summary function defined.\")\n"
      ],
      "metadata": {
        "id": "565c3354-e760-4d12-a101-4cd3ddb2eaed"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0d621370-c763-41db-9747-4b15ebe3e719",
      "cell_type": "code",
      "source": [
        "#This cell is where we test performance for different combinations of lambda penalty values for turnover, risk, and arbitrage difficulty\n",
        "lambda_configs = [ #Array to store all the combinations of lambdas we want to try\n",
        "    (0.6, 0.5, 0.6), #Neutral penalties (pension fund-ish)\n",
        "    (0.3, 0.2, 0.3), #Liberal penalties (hedge fund-ish)\n",
        "    (1.0, 1.2, 1.0), #Conservative penalties (retail investor-ish)\n",
        "    (1.0, 0.7, 1.5), #Conservative penalties (conservative mutual fund-ish)\n",
        "]\n",
        "\n",
        "#Now we will add the cartesian product of the following possible lambdas\n",
        "lambda_t_values = [0.3, 0.6, 1.0] #Turnover lambdas we want to try\n",
        "lambda_r_values = [0.2, 0.5, 1.0] #Risk lambdas we want to try\n",
        "lambda_a_values = [0.3, 0.6, 1.0] #Arb difficulty lambdas we want to try\n",
        "\n",
        "lambda_configs = lambda_configs + list(product(lambda_t_values, lambda_r_values, lambda_a_values)) #Add the cartestian product of the possible lambdas 2 x 3 x 3 = 18 additional configs, 22 total\n",
        "\n",
        "for i, (lt, lr, la) in enumerate(lambda_configs): #Iterate through each lambda combination'\n",
        "    filename = get_filename(lt, lr, la) #Make filename string using these lambdas\n",
        "\n",
        "    print(f\"Running λ_t={lt}, λ_r={lr}, λ_a={la}\")\n",
        "\n",
        "    #Check if file already exists\n",
        "    if os.path.exists(filename):\n",
        "        print(filename, \"already exists. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    returns_df = run_portfolio_optimization(lt, lr, la, full_oos=False, test_years=[2004, 2005, 2008, 2009]) #Run portfolio optimization for this lambda combination\n",
        "    #We pick 2004, 2005, 2008, 2009 to test on a mix of smooth and rocky market conditions\n",
        "\n",
        "    summary_df = compute_optimized_summary(returns_df) #Make summary table for this lambda combination\n",
        "\n",
        "    result = { #Construct result dictionary containing which lambdas we're on and their corresponding summary table\n",
        "        'lambdas': {'lambda_t': lt, 'lambda_r': lr, 'lambda_a': la},\n",
        "        'summary': summary_df\n",
        "    }\n",
        "\n",
        "    with open(filename, \"wb\") as f: #Store result dictionary into a .pkl file\n",
        "        pickle.dump(result, f)\n",
        "\n",
        "    print(filename, \" was saved successfully.\")"
      ],
      "metadata": {
        "id": "0d621370-c763-41db-9747-4b15ebe3e719"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c3a40eef-5951-4712-b136-cd957975b32a",
      "cell_type": "code",
      "source": [
        "folder_path = r\"YOUR_FOLDER_PATH_STORING_PKL_FILES\"\n",
        "file_names = os.listdir(folder_path) #Put all .pkl files in a folder and list of all them here\n",
        "\n",
        "results = {} #results dictionary to store results of all lambda combinations\n",
        "for file_name in file_names: #Iterate through each .pkl filename\n",
        "    full_path = os.path.join(folder_path, file_name) #Get full path\n",
        "    with open(full_path, \"rb\") as f: #Open file\n",
        "        results[file_name] = pickle.load(f) #Add to results dictionary\n",
        "\n",
        "all_summaries = [] #Build metrics DataFrame\n",
        "for file_name, result in results.items(): #Iterate through each lambda's value in results\n",
        "    s = result['summary'].iloc[0].copy() #Get summary section\n",
        "    s['file_name'] = file_name  #Store file name as plain string\n",
        "    all_summaries.append(s)\n",
        "\n",
        "df_metrics = pd.DataFrame(all_summaries) #Make actual DF\n",
        "\n",
        "#For normalizing metrics\n",
        "metric_info = {\n",
        "    'realized_mean': {'invert': False}, #Better if higher\n",
        "    'sharpe_ratio': {'invert': False}, #Better if higher\n",
        "    'avg_ex_ante_risk': {'invert': True}, #Better if lower\n",
        "    'avg_turnover': {'invert': True}, #Better if lower\n",
        "    'avg_arb_exposure': {'invert': True}, #Better if lower\n",
        "}\n",
        "\n",
        "for metric, info in metric_info.items(): #Iterate through each metric\n",
        "    col = df_metrics[metric] #Get the column for the metric\n",
        "    normalized = (col - col.min()) / (col.max() - col.min()) #Normalize\n",
        "    if info['invert']:\n",
        "        normalized = 1 - normalized #Invert if necessary\n",
        "    df_metrics[f'norm_{metric}'] = normalized #Store\n",
        "\n",
        "#Dictionary mapping investor types to their relative priority (1–10) for each portfolio performance metric.\n",
        "#These weights reflect how much each investor group values metrics like realized return, Sharpe ratio,\n",
        "#ex-ante risk, turnover, and arbitrage exposure. Higher numbers indicate stronger preferences.\n",
        "investor_priorities = {\n",
        "    'retail_investor': {\n",
        "        'realized_mean': 3,\n",
        "        'sharpe_ratio': 5,\n",
        "        'avg_ex_ante_risk': 9,\n",
        "        'avg_turnover': 8,\n",
        "        'avg_arb_exposure': 6\n",
        "    },\n",
        "    'pension_fund': {\n",
        "        'realized_mean': 6,\n",
        "        'sharpe_ratio': 8,\n",
        "        'avg_ex_ante_risk': 6,\n",
        "        'avg_turnover': 5,\n",
        "        'avg_arb_exposure': 3\n",
        "    },\n",
        "    'mutual_fund': {\n",
        "        'realized_mean': 5,\n",
        "        'sharpe_ratio': 6,\n",
        "        'avg_ex_ante_risk': 5,\n",
        "        'avg_turnover': 5,\n",
        "        'avg_arb_exposure': 8\n",
        "    },\n",
        "    'hedge_fund': {\n",
        "        'realized_mean': 10,\n",
        "        'sharpe_ratio': 10,\n",
        "        'avg_ex_ante_risk': 2,\n",
        "        'avg_turnover': 2,\n",
        "        'avg_arb_exposure': 2\n",
        "    }\n",
        "}\n",
        "\n",
        "def score_config(row, group): #Define scoring function\n",
        "    weights = investor_priorities[group] #Get weights for investor\n",
        "    score = 0 #Set score to 0\n",
        "    for metric in weights: #Iterate through each of this group's metric and weight\n",
        "        score += weights[metric] * row[f'norm_{metric}'] #Add weighted score for this metric to overall score\n",
        "    return score #Return final score\n",
        "\n",
        "df_metrics.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for group in investor_priorities.keys(): #Iterate through each group\n",
        "    df_metrics[f'{group}_score'] = df_metrics.apply(lambda row: score_config(row, group), axis=1) #Get final score for each lambda combination for this group\n",
        "\n",
        "    best_idx = df_metrics[f'{group}_score'].idxmax() #Get best score\n",
        "    best_file = df_metrics.loc[best_idx, 'file_name']  #Get filename for best score\n",
        "\n",
        "    print(f\"\\nBest config for {group}:\") #Print results\n",
        "    print(f\"Lambdas: {results[best_file]['lambdas']}\")\n",
        "    print(f\"Summary:\\n{results[best_file]['summary']}\")\n",
        "\n",
        "#Extract lambdas from filename for clarity in the table\n",
        "df_metrics['lambda_t'] = df_metrics['file_name'].apply(lambda x: float(x.split('_r')[0].replace('lambda_t', '').replace('.pkl', '').replace('a', '')))\n",
        "df_metrics['lambda_r'] = df_metrics['file_name'].apply(lambda x: float(x.split('_r')[1].split('_a')[0]))\n",
        "df_metrics['lambda_a'] = df_metrics['file_name'].apply(lambda x: float(x.split('_a')[1].replace('.pkl', '').replace('\\n', '')))\n",
        "\n",
        "columns_to_display = [ #Keep only relevant columns for the final output\n",
        "    'lambda_t', 'lambda_r', 'lambda_a',\n",
        "    'realized_mean', 'sharpe_ratio', 'avg_ex_ante_risk',\n",
        "    'avg_turnover', 'avg_arb_exposure'\n",
        "]\n",
        "\n",
        "df_metrics = df_metrics.sort_values(by=['lambda_t', 'lambda_r', 'lambda_a']).reset_index(drop=True) #Sort\n",
        "\n",
        "print(\"\\n=== Results Table: All Lambda Configurations ===\\n\")\n",
        "print(df_metrics[columns_to_display].to_string(index=False)) #Print full table\n",
        "\n",
        "#Thank you! Please reach out with any questions at reginaldjhyde@gmail.com or hyde9698@stthomas.edu"
      ],
      "metadata": {
        "trusted": true,
        "id": "c3a40eef-5951-4712-b136-cd957975b32a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2c17cbbe-17d4-428d-920b-12e6e162a68c",
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2c17cbbe-17d4-428d-920b-12e6e162a68c"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}